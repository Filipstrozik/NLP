{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from peft import AutoPeftModelForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"clarin-knext/wsd_polish_datasets\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'tokens', 'phrases', 'wsd'],\n",
       "        num_rows: 7848\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features['tokens'].feature['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Zatem to pani siostra o niego prosiła? Skądże, nigdy nie słyszałam, żeby go używała. Zwykłyśmy obywać się bez służących. W istocie, wydaje się niepotrzebnym umieszczanie tak porządnego sznura w tym miejscu. Bądźcie teraz tak uprzejmi i dajcie mi kilka minut na bliższe zapoznanie się z podłogą. Holmes przypadł do podłogi, pełzał w te i wewte ze szkłem powiększającym w dłoni, ze skupieniem badając przerwy między deskami. Następnie w podobny sposób przyjrzał się deskom na ścianach komnaty. Potem podszedł do łóżka i przez jakiś czas przyglądał się uważnie zarówno jemu, jak i przylegającej doń ścianie. Wreszcie chwycił sznur i energicznie go pociągnął. Ależ to przecież atrapa - powiedział. Nie dzwoni? Bynajmniej, nie jest nawet połączony z drucikiem. Niezwykle interesujące. Zwróćcie uwagę, proszę, że jest podpięty do haka tuż nad niewielkim otworem pełniącym funkcję wywietrznika. Toż to niedorzeczne! Nie zauważyłam tego wcześniej. Bardzo dziwne - wymamrotał Holmes, pociągając za sznur - W tym pokoju znajduje się jedna lub dwie osobliwe rzeczy. Na przykład, jakim głupcem musiał być budowniczy, skoro przebił wywietrznik do sąsiedniego pokoju, kiedy równie dobrze mógł wybić go na zewnątrz. To również jedna z nowości - stwierdziła dama. Wykonany w tym samym czasie co sznur? - zauważył Holmes. Tak, dokonano wtedy kilku niewielkich zmian. Nad wyraz ciekawe te zmiany. Atrapy dzwonków dla służby i wywietrzniki, które nie działają. Za pani pozwoleniem, panno Stoner, przejdziemy teraz do oględzin ostatniego pokoju.', 'tokens': {'position': [[0, 5], [6, 8], [9, 13], [14, 21], [22, 23], [24, 29], [30, 37], [37, 38], [39, 45], [45, 46], [47, 52], [53, 56], [57, 65], [65, 66], [66, 67], [68, 72], [73, 75], [76, 83], [83, 84], [85, 91], [91, 94], [95, 101], [102, 105], [106, 109], [110, 119], [119, 120], [121, 122], [123, 130], [130, 131], [132, 138], [139, 142], [143, 156], [157, 169], [170, 173], [174, 184], [185, 191], [192, 193], [194, 197], [198, 205], [205, 206], [207, 214], [215, 220], [221, 224], [225, 233], [234, 235], [236, 242], [243, 245], [246, 251], [252, 257], [258, 260], [261, 268], [269, 279], [280, 283], [284, 285], [286, 293], [293, 294], [295, 301], [302, 310], [311, 313], [314, 321], [321, 322], [323, 329], [330, 331], [332, 334], [335, 336], [337, 342], [343, 345], [346, 352], [353, 367], [368, 369], [370, 375], [375, 376], [377, 379], [380, 390], [391, 398], [399, 406], [407, 413], [414, 421], [421, 422], [423, 432], [433, 434], [435, 442], [443, 449], [450, 459], [460, 463], [464, 470], [471, 473], [474, 482], [483, 490], [490, 491], [492, 497], [498, 506], [507, 509], [510, 515], [516, 517], [518, 523], [524, 529], [530, 534], [535, 545], [546, 549], [550, 557], [558, 565], [566, 570], [570, 571], [572, 575], [576, 577], [578, 591], [592, 594], [594, 595], [596, 603], [603, 604], [605, 613], [614, 621], [622, 627], [628, 629], [630, 641], [642, 644], [645, 654], [654, 655], [656, 660], [661, 663], [664, 672], [673, 679], [680, 681], [682, 692], [692, 693], [694, 697], [698, 704], [704, 705], [706, 716], [716, 717], [718, 721], [722, 726], [727, 732], [733, 742], [743, 744], [745, 754], [754, 755], [756, 765], [766, 778], [778, 779], [780, 788], [789, 794], [794, 795], [796, 802], [802, 803], [804, 806], [807, 811], [812, 820], [821, 823], [824, 828], [829, 832], [833, 836], [837, 847], [848, 855], [856, 865], [866, 873], [874, 886], [886, 887], [888, 891], [892, 894], [895, 907], [907, 908], [909, 912], [913, 922], [922, 923], [924, 928], [929, 938], [938, 939], [940, 946], [947, 953], [954, 955], [956, 966], [967, 973], [973, 974], [975, 985], [986, 988], [989, 994], [995, 996], [997, 998], [999, 1002], [1003, 1009], [1010, 1018], [1019, 1022], [1023, 1028], [1029, 1032], [1033, 1037], [1038, 1046], [1047, 1053], [1053, 1054], [1055, 1057], [1058, 1066], [1066, 1067], [1068, 1073], [1074, 1081], [1082, 1088], [1089, 1092], [1093, 1103], [1103, 1104], [1105, 1110], [1111, 1118], [1119, 1130], [1131, 1133], [1134, 1145], [1146, 1152], [1152, 1153], [1154, 1159], [1160, 1166], [1167, 1173], [1174, 1178], [1179, 1184], [1185, 1187], [1188, 1190], [1191, 1199], [1199, 1200], [1201, 1203], [1204, 1211], [1212, 1217], [1218, 1219], [1220, 1227], [1228, 1229], [1230, 1241], [1242, 1246], [1246, 1247], [1248, 1256], [1257, 1258], [1259, 1262], [1263, 1268], [1269, 1275], [1276, 1278], [1279, 1284], [1284, 1285], [1286, 1287], [1288, 1296], [1297, 1303], [1303, 1304], [1305, 1308], [1308, 1309], [1310, 1318], [1319, 1324], [1325, 1330], [1331, 1342], [1343, 1348], [1348, 1349], [1350, 1353], [1354, 1359], [1360, 1367], [1368, 1370], [1371, 1377], [1377, 1378], [1379, 1385], [1386, 1394], [1395, 1398], [1399, 1405], [1406, 1407], [1408, 1420], [1420, 1421], [1422, 1427], [1428, 1431], [1432, 1440], [1440, 1441], [1442, 1444], [1445, 1449], [1450, 1461], [1461, 1462], [1463, 1468], [1469, 1475], [1475, 1476], [1477, 1488], [1489, 1494], [1495, 1497], [1498, 1506], [1507, 1517], [1518, 1524], [1524, 1525]], 'orth': ['Zatem', 'to', 'pani', 'siostra', 'o', 'niego', 'prosiła', '?', 'Skądże', ',', 'nigdy', 'nie', 'słyszała', 'm', ',', 'żeby', 'go', 'używała', '.', 'Zwykły', 'śmy', 'obywać', 'się', 'bez', 'służących', '.', 'W', 'istocie', ',', 'wydaje', 'się', 'niepotrzebnym', 'umieszczanie', 'tak', 'porządnego', 'sznura', 'w', 'tym', 'miejscu', '.', 'Bądźcie', 'teraz', 'tak', 'uprzejmi', 'i', 'dajcie', 'mi', 'kilka', 'minut', 'na', 'bliższe', 'zapoznanie', 'się', 'z', 'podłogą', '.', 'Holmes', 'przypadł', 'do', 'podłogi', ',', 'pełzał', 'w', 'te', 'i', 'wewte', 'ze', 'szkłem', 'powiększającym', 'w', 'dłoni', ',', 'ze', 'skupieniem', 'badając', 'przerwy', 'między', 'deskami', '.', 'Następnie', 'w', 'podobny', 'sposób', 'przyjrzał', 'się', 'deskom', 'na', 'ścianach', 'komnaty', '.', 'Potem', 'podszedł', 'do', 'łóżka', 'i', 'przez', 'jakiś', 'czas', 'przyglądał', 'się', 'uważnie', 'zarówno', 'jemu', ',', 'jak', 'i', 'przylegającej', 'do', 'ń', 'ścianie', '.', 'Wreszcie', 'chwycił', 'sznur', 'i', 'energicznie', 'go', 'pociągnął', '.', 'Ależ', 'to', 'przecież', 'atrapa', '-', 'powiedział', '.', 'Nie', 'dzwoni', '?', 'Bynajmniej', ',', 'nie', 'jest', 'nawet', 'połączony', 'z', 'drucikiem', '.', 'Niezwykle', 'interesujące', '.', 'Zwróćcie', 'uwagę', ',', 'proszę', ',', 'że', 'jest', 'podpięty', 'do', 'haka', 'tuż', 'nad', 'niewielkim', 'otworem', 'pełniącym', 'funkcję', 'wywietrznika', '.', 'Toż', 'to', 'niedorzeczne', '!', 'Nie', 'zauważyła', 'm', 'tego', 'wcześniej', '.', 'Bardzo', 'dziwne', '-', 'wymamrotał', 'Holmes', ',', 'pociągając', 'za', 'sznur', '-', 'W', 'tym', 'pokoju', 'znajduje', 'się', 'jedna', 'lub', 'dwie', 'osobliwe', 'rzeczy', '.', 'Na', 'przykład', ',', 'jakim', 'głupcem', 'musiał', 'być', 'budowniczy', ',', 'skoro', 'przebił', 'wywietrznik', 'do', 'sąsiedniego', 'pokoju', ',', 'kiedy', 'równie', 'dobrze', 'mógł', 'wybić', 'go', 'na', 'zewnątrz', '.', 'To', 'również', 'jedna', 'z', 'nowości', '-', 'stwierdziła', 'dama', '.', 'Wykonany', 'w', 'tym', 'samym', 'czasie', 'co', 'sznur', '?', '-', 'zauważył', 'Holmes', '.', 'Tak', ',', 'dokonano', 'wtedy', 'kilku', 'niewielkich', 'zmian', '.', 'Nad', 'wyraz', 'ciekawe', 'te', 'zmiany', '.', 'Atrapy', 'dzwonków', 'dla', 'służby', 'i', 'wywietrzniki', ',', 'które', 'nie', 'działają', '.', 'Za', 'pani', 'pozwoleniem', ',', 'panno', 'Stoner', ',', 'przejdziemy', 'teraz', 'do', 'oględzin', 'ostatniego', 'pokoju', '.'], 'lemma': ['zatem', 'to', 'pani', 'siostra', 'o', 'on', 'prosić', '?', 'skądże', ',', 'nigdy', 'nie', 'słyszeć', 'być', ',', 'żeby', 'on', 'używać', '.', 'zwyknąć', 'być', 'obywać', 'się', 'bez', 'służący', '.', 'w', 'istota', ',', 'wydawać', 'się', 'niepotrzebny', 'umieszczać', 'tak', 'porządny', 'sznur', 'w', 'ten', 'miejsce', '.', 'być', 'teraz', 'tak', 'uprzejmy', 'i', 'dać', 'ja', 'kilka', 'minuta', 'na', 'bliski', 'zapoznać', 'się', 'z', 'podłoga', '.', 'Holmes', 'przypaść', 'do', 'podłoga', ',', 'pełzać', 'w', 'ten', 'i', 'wewte', 'z', 'szkło', 'powiększać', 'w', 'dłoń', ',', 'z', 'skupienie', 'badać', 'przerwa', 'między', 'deska', '.', 'następnie', 'w', 'podobny', 'sposób', 'przyjrzeć', 'się', 'deska', 'na', 'ściana', 'komnata', '.', 'potem', 'podejść', 'do', 'łóżko', 'i', 'przez', 'jakiś', 'czas', 'przyglądać', 'się', 'uważnie', 'zarówno', 'on', ',', 'jak', 'i', 'przylegający', 'donia', 'on', 'ściana', '.', 'wreszcie', 'chwycić', 'sznur', 'i', 'energicznie', 'on', 'pociągnąć', '.', 'ależ', 'to', 'przecież', 'atrapa', '-', 'powiedzieć', '.', 'nie', 'dzwonić', '?', 'bynajmniej', ',', 'nie', 'być', 'nawet', 'połączyć', 'z', 'drucik', '.', 'niezwykle', 'interesujący', '.', 'zwrócić', 'uwaga', ',', 'prosić', ',', 'że', 'być', 'podpiąć', 'do', 'hak', 'tuż', 'nad', 'niewielki', 'otwór', 'pełnić', 'funkcja', 'wywietrznik', '.', 'toż', 'to', 'niedorzeczny', '!', 'nie', 'zauważyć', 'być', 'to', 'wczesno', '.', 'bardzo', 'dziwny', '-', 'wymamrotać', 'Holmes', ',', 'pociągać', 'za', 'sznur', '-', 'w', 'ten', 'pokój', 'znajdować', 'się', 'jeden', 'lub', 'dwa', 'osobliwy', 'rzecz', '.', 'na', 'przykład', ',', 'jaki', 'głupiec', 'musieć', 'być', 'budowniczy', ',', 'skoro', 'przebić', 'wywietrznik', 'do', 'sąsiedni', 'pokój', ',', 'kiedy', 'równie', 'dobrze', 'móc', 'wybić', 'on', 'na', 'zewnątrz', '.', 'to', 'również', 'jeden', 'z', 'nowość', '-', 'stwierdzić', 'dama', '.', 'wykonać', 'w', 'ten', 'sam', 'czas', 'CO', 'sznur', '?', '-', 'zauważyć', 'Holmes', '.', 'tak', ',', 'dokonać', 'wtedy', 'kilka', 'niewielki', 'zmiana', '.', 'nad', 'wyraz', 'ciekawy', 'ten', 'zmiana', '.', 'atrapa', 'dzwonek', 'dla', 'służba', 'i', 'wywietrznik', ',', 'który', 'nie', 'działać', '.', 'za', 'pani', 'pozwolenie', ',', 'Panna', 'stoner', ',', 'przejść', 'teraz', 'do', 'oględziny', 'ostatni', 'pokój', '.'], 'pos': ['conj', 'verb', 'noun', 'noun', 'prep', 'ppron3', 'verb', 'interp', 'adv', 'interp', 'adv', 'qub', 'verb', 'aglt', 'interp', 'comp', 'ppron3', 'verb', 'interp', 'verb', 'aglt', 'verb', 'qub', 'prep', 'noun', 'interp', 'prep', 'noun', 'interp', 'verb', 'qub', 'adj', 'verb', 'adv', 'adj', 'noun', 'prep', 'adj', 'noun', 'interp', 'verb', 'adv', 'adv', 'adj', 'conj', 'verb', 'ppron12', 'num', 'noun', 'prep', 'adj', 'verb', 'qub', 'prep', 'noun', 'interp', 'noun', 'verb', 'prep', 'noun', 'interp', 'verb', 'prep', 'adj', 'qub', 'burk', 'prep', 'noun', 'verb', 'prep', 'noun', 'interp', 'prep', 'noun', 'verb', 'noun', 'prep', 'noun', 'interp', 'adv', 'prep', 'adj', 'noun', 'verb', 'qub', 'noun', 'prep', 'noun', 'noun', 'interp', 'adv', 'verb', 'prep', 'noun', 'conj', 'prep', 'adj', 'noun', 'verb', 'qub', 'adv', 'conj', 'ppron3', 'interp', 'conj', 'conj', 'adj', 'noun', 'ppron3', 'noun', 'interp', 'qub', 'verb', 'noun', 'conj', 'adv', 'ppron3', 'verb', 'interp', 'qub', 'verb', 'qub', 'noun', 'interp', 'verb', 'interp', 'qub', 'verb', 'interp', 'qub', 'interp', 'qub', 'verb', 'qub', 'verb', 'prep', 'noun', 'interp', 'adv', 'adj', 'interp', 'verb', 'noun', 'interp', 'verb', 'interp', 'comp', 'verb', 'verb', 'prep', 'noun', 'qub', 'prep', 'adj', 'noun', 'verb', 'noun', 'noun', 'interp', 'qub', 'noun', 'adj', 'interp', 'qub', 'verb', 'aglt', 'noun', 'adv', 'interp', 'adv', 'adj', 'interp', 'verb', 'noun', 'interp', 'verb', 'prep', 'noun', 'interp', 'prep', 'adj', 'noun', 'verb', 'qub', 'adj', 'conj', 'num', 'adj', 'noun', 'interp', 'prep', 'noun', 'interp', 'adj', 'noun', 'verb', 'verb', 'adj', 'interp', 'comp', 'verb', 'noun', 'prep', 'adj', 'noun', 'interp', 'adv', 'adv', 'adv', 'verb', 'verb', 'ppron3', 'prep', 'adv', 'interp', 'verb', 'qub', 'adj', 'prep', 'noun', 'interp', 'verb', 'noun', 'interp', 'verb', 'prep', 'adj', 'adj', 'noun', 'noun', 'noun', 'interp', 'interp', 'verb', 'noun', 'interp', 'adv', 'interp', 'verb', 'adv', 'num', 'adj', 'noun', 'interp', 'prep', 'noun', 'adj', 'adj', 'noun', 'interp', 'noun', 'noun', 'prep', 'noun', 'conj', 'noun', 'interp', 'adj', 'qub', 'verb', 'interp', 'prep', 'noun', 'noun', 'interp', 'noun', 'noun', 'interp', 'verb', 'adv', 'prep', 'noun', 'adj', 'noun', 'interp']}, 'phrases': {'indices': [[21, 22], [26, 27], [29, 30], [83, 84], [98, 99], [244, 245]], 'head': [21, 27, 29, 83, 98, 245], 'lemma': ['obywać się', 'w istocie', 'wydawać się', 'przyjrzeć się', 'przyglądać się', 'nad wyraz']}, 'wsd': {'index': [2, 3, 6, 10, 12, 17, 19, 21, 24, 26, 27, 29, 31, 32, 34, 35, 38, 40, 41, 43, 45, 48, 50, 54, 57, 59, 61, 67, 68, 70, 73, 74, 75, 77, 79, 81, 82, 83, 85, 87, 88, 90, 91, 93, 97, 98, 100, 106, 109, 112, 113, 115, 117, 122, 124, 127, 132, 136, 138, 139, 144, 147, 148, 150, 153, 154, 155, 156, 157, 161, 164, 169, 170, 172, 175, 177, 181, 187, 188, 194, 196, 200, 201, 203, 204, 210, 217, 219, 221, 222, 224, 228, 230, 233, 238, 241, 242, 244, 245, 246, 248, 250, 253, 255, 259, 262, 263, 265, 268, 269, 271, 272, 273], 'plWN_syn_id': ['f4347913-aac4-11ed-aae5-0242ac130002', 'f3c18675-aac4-11ed-aae5-0242ac130002', 'f39e2ec4-aac4-11ed-aae5-0242ac130002', '011dbe5a-aac5-11ed-aae5-0242ac130002', 'f3a49e44-aac4-11ed-aae5-0242ac130002', 'f4d92a92-aac4-11ed-aae5-0242ac130002', 'f5434086-aac4-11ed-aae5-0242ac130002', 'f54eca27-aac4-11ed-aae5-0242ac130002', 'f3ba7583-aac4-11ed-aae5-0242ac130002', '01bed93c-aac5-11ed-aae5-0242ac130002', '01bed93c-aac5-11ed-aae5-0242ac130002', 'f3d72de8-aac4-11ed-aae5-0242ac130002', 'f3afe5bb-aac4-11ed-aae5-0242ac130002', 'f3db9c80-aac4-11ed-aae5-0242ac130002', 'f3d0e84b-aac4-11ed-aae5-0242ac130002', 'f3cda992-aac4-11ed-aae5-0242ac130002', 'f3b34d98-aac4-11ed-aae5-0242ac130002', 'f85ca336-aac4-11ed-aae5-0242ac130002', '01a7d6a1-aac5-11ed-aae5-0242ac130002', '01347442-aac5-11ed-aae5-0242ac130002', 'f4ce13bb-aac4-11ed-aae5-0242ac130002', 'f3b6e3a8-aac4-11ed-aae5-0242ac130002', '00b9c9cf-aac5-11ed-aae5-0242ac130002', 'f3cd377a-aac4-11ed-aae5-0242ac130002', 'f54ecac9-aac4-11ed-aae5-0242ac130002', 'f3cd377a-aac4-11ed-aae5-0242ac130002', 'f3a168ca-aac4-11ed-aae5-0242ac130002', 'fe26ca3f-aac4-11ed-aae5-0242ac130002', 'f81cc66a-aac4-11ed-aae5-0242ac130002', 'f52a0b08-aac4-11ed-aae5-0242ac130002', 'f3903b0c-aac4-11ed-aae5-0242ac130002', '02432ae7-aac5-11ed-aae5-0242ac130002', 'f3b73031-aac4-11ed-aae5-0242ac130002', 'f3cd06ca-aac4-11ed-aae5-0242ac130002', '01a4a613-aac5-11ed-aae5-0242ac130002', 'f3d0b9e7-aac4-11ed-aae5-0242ac130002', 'f3b7454c-aac4-11ed-aae5-0242ac130002', 'f39ed305-aac4-11ed-aae5-0242ac130002', 'f3cd06ca-aac4-11ed-aae5-0242ac130002', 'f3c5c57c-aac4-11ed-aae5-0242ac130002', 'f3f40d15-aac4-11ed-aae5-0242ac130002', '01a4a2cd-aac5-11ed-aae5-0242ac130002', 'f39a2739-aac4-11ed-aae5-0242ac130002', 'f3c84fe6-aac4-11ed-aae5-0242ac130002', 'f46b31a8-aac4-11ed-aae5-0242ac130002', 'f39ece0e-aac4-11ed-aae5-0242ac130002', '009c3c37-aac5-11ed-aae5-0242ac130002', 'f55e3c29-aac4-11ed-aae5-0242ac130002', 'f3c5c57c-aac4-11ed-aae5-0242ac130002', 'f36010fb-aac4-11ed-aae5-0242ac130002', 'f3cda992-aac4-11ed-aae5-0242ac130002', '009e15a9-aac5-11ed-aae5-0242ac130002', 'f39a1edc-aac4-11ed-aae5-0242ac130002', 'f40f0384-aac4-11ed-aae5-0242ac130002', 'f39ab3de-aac4-11ed-aae5-0242ac130002', 'f53f975c-aac4-11ed-aae5-0242ac130002', 'f85ca336-aac4-11ed-aae5-0242ac130002', 'f3ec9fb4-aac4-11ed-aae5-0242ac130002', '00ac945c-aac5-11ed-aae5-0242ac130002', 'ffb13a9c-aac4-11ed-aae5-0242ac130002', 'f7f762af-aac4-11ed-aae5-0242ac130002', 'f85ca336-aac4-11ed-aae5-0242ac130002', 'f48751f5-aac4-11ed-aae5-0242ac130002', 'f3ec2a9d-aac4-11ed-aae5-0242ac130002', 'f3d30788-aac4-11ed-aae5-0242ac130002', 'f3b4cb13-aac4-11ed-aae5-0242ac130002', 'f3a2a117-aac4-11ed-aae5-0242ac130002', 'f3b741bd-aac4-11ed-aae5-0242ac130002', 'f3ea8af6-aac4-11ed-aae5-0242ac130002', 'f7f32d79-aac4-11ed-aae5-0242ac130002', 'f42c253a-aac4-11ed-aae5-0242ac130002', '00b909a2-aac5-11ed-aae5-0242ac130002', 'f3609c6e-aac4-11ed-aae5-0242ac130002', 'f3d507f3-aac4-11ed-aae5-0242ac130002', 'f4bf44d9-aac4-11ed-aae5-0242ac130002', 'f3cda992-aac4-11ed-aae5-0242ac130002', 'f3b4c1a9-aac4-11ed-aae5-0242ac130002', 'f3609c6e-aac4-11ed-aae5-0242ac130002', 'f7eeaaf9-aac4-11ed-aae5-0242ac130002', 'f5755177-aac4-11ed-aae5-0242ac130002', 'f85ca4f4-aac4-11ed-aae5-0242ac130002', 'f4ef62eb-aac4-11ed-aae5-0242ac130002', 'f3ea8af6-aac4-11ed-aae5-0242ac130002', 'f3d51c70-aac4-11ed-aae5-0242ac130002', 'f3b4c1a9-aac4-11ed-aae5-0242ac130002', '022e823d-aac5-11ed-aae5-0242ac130002', 'f38cf433-aac4-11ed-aae5-0242ac130002', 'ff3b5378-aac4-11ed-aae5-0242ac130002', 'f3a2cafc-aac4-11ed-aae5-0242ac130002', 'f360377d-aac4-11ed-aae5-0242ac130002', '02185cbb-aac5-11ed-aae5-0242ac130002', 'f46b3274-aac4-11ed-aae5-0242ac130002', 'f3cda992-aac4-11ed-aae5-0242ac130002', 'f4c5b3bf-aac4-11ed-aae5-0242ac130002', 'f50bcb7c-aac4-11ed-aae5-0242ac130002', '0121bd15-aac5-11ed-aae5-0242ac130002', 'f4796381-aac4-11ed-aae5-0242ac130002', '0130117a-aac5-11ed-aae5-0242ac130002', '0130117a-aac5-11ed-aae5-0242ac130002', 'f3601783-aac4-11ed-aae5-0242ac130002', 'f4796381-aac4-11ed-aae5-0242ac130002', 'f40f0384-aac4-11ed-aae5-0242ac130002', 'f3ffe81c-aac4-11ed-aae5-0242ac130002', 'f3ea8af6-aac4-11ed-aae5-0242ac130002', 'f49d3863-aac4-11ed-aae5-0242ac130002', 'f4347913-aac4-11ed-aae5-0242ac130002', 'f7ca3092-aac4-11ed-aae5-0242ac130002', 'f5b049af-aac4-11ed-aae5-0242ac130002', 'f4c9083f-aac4-11ed-aae5-0242ac130002', '01a7d6a1-aac5-11ed-aae5-0242ac130002', 'f40da05a-aac4-11ed-aae5-0242ac130002', 'f399485a-aac4-11ed-aae5-0242ac130002', 'f3b4c1a9-aac4-11ed-aae5-0242ac130002'], 'plWN_lex_id': ['d726ace0-aac4-11ed-aae5-0242ac130002', 'd51563c3-aac4-11ed-aae5-0242ac130002', 'd4a2d193-aac4-11ed-aae5-0242ac130002', 'f23a1607-aac4-11ed-aae5-0242ac130002', 'd4b3b869-aac4-11ed-aae5-0242ac130002', 'd4def193-aac4-11ed-aae5-0242ac130002', 'd9126ea4-aac4-11ed-aae5-0242ac130002', 'd9183eed-aac4-11ed-aae5-0242ac130002', 'd4b3b38b-aac4-11ed-aae5-0242ac130002', 'f2bd6291-aac4-11ed-aae5-0242ac130002', 'f2bd6291-aac4-11ed-aae5-0242ac130002', 'd9166c4d-aac4-11ed-aae5-0242ac130002', 'd511b294-aac4-11ed-aae5-0242ac130002', 'd4de0912-aac4-11ed-aae5-0242ac130002', 'd49ddb65-aac4-11ed-aae5-0242ac130002', 'd50b5644-aac4-11ed-aae5-0242ac130002', 'd48d71e4-aac4-11ed-aae5-0242ac130002', 'dd906cc4-aac4-11ed-aae5-0242ac130002', 'f29c64a0-aac4-11ed-aae5-0242ac130002', 'f259685d-aac4-11ed-aae5-0242ac130002', 'd89c127f-aac4-11ed-aae5-0242ac130002', 'd511692c-aac4-11ed-aae5-0242ac130002', 'f1b96a86-aac4-11ed-aae5-0242ac130002', 'd49cb354-aac4-11ed-aae5-0242ac130002', 'd9184f6d-aac4-11ed-aae5-0242ac130002', 'd49cb354-aac4-11ed-aae5-0242ac130002', 'd49953d9-aac4-11ed-aae5-0242ac130002', 'ed51e31d-aac4-11ed-aae5-0242ac130002', 'dc8ab17b-aac4-11ed-aae5-0242ac130002', 'd90a38c4-aac4-11ed-aae5-0242ac130002', 'd4afa31c-aac4-11ed-aae5-0242ac130002', 'd462a957-aac4-11ed-aae5-0242ac130002', 'd4a482aa-aac4-11ed-aae5-0242ac130002', 'd4698997-aac4-11ed-aae5-0242ac130002', 'f2759518-aac4-11ed-aae5-0242ac130002', 'd49c83b6-aac4-11ed-aae5-0242ac130002', 'd4b062ed-aac4-11ed-aae5-0242ac130002', 'd4a696e2-aac4-11ed-aae5-0242ac130002', 'd4698997-aac4-11ed-aae5-0242ac130002', 'd4adc78c-aac4-11ed-aae5-0242ac130002', 'd540601c-aac4-11ed-aae5-0242ac130002', 'f27588c7-aac4-11ed-aae5-0242ac130002', 'd49b4d1b-aac4-11ed-aae5-0242ac130002', 'd4e74d4d-aac4-11ed-aae5-0242ac130002', 'd760b6ae-aac4-11ed-aae5-0242ac130002', 'd4a621d8-aac4-11ed-aae5-0242ac130002', 'f02b9f7b-aac4-11ed-aae5-0242ac130002', 'd923a755-aac4-11ed-aae5-0242ac130002', 'd4adc78c-aac4-11ed-aae5-0242ac130002', 'd4687bb9-aac4-11ed-aae5-0242ac130002', 'd50b5644-aac4-11ed-aae5-0242ac130002', 'f01339cd-aac4-11ed-aae5-0242ac130002', 'd49b0c20-aac4-11ed-aae5-0242ac130002', 'd5c4d509-aac4-11ed-aae5-0242ac130002', 'd514ba69-aac4-11ed-aae5-0242ac130002', 'd9108e75-aac4-11ed-aae5-0242ac130002', 'dd906cc4-aac4-11ed-aae5-0242ac130002', 'd5321ca0-aac4-11ed-aae5-0242ac130002', 'f14615ca-aac4-11ed-aae5-0242ac130002', 'efa0c202-aac4-11ed-aae5-0242ac130002', 'dc40348f-aac4-11ed-aae5-0242ac130002', 'dd906cc4-aac4-11ed-aae5-0242ac130002', 'd76a23c7-aac4-11ed-aae5-0242ac130002', 'd53186c0-aac4-11ed-aae5-0242ac130002', 'd51950f1-aac4-11ed-aae5-0242ac130002', 'd49782b2-aac4-11ed-aae5-0242ac130002', 'd499518f-aac4-11ed-aae5-0242ac130002', 'd483503a-aac4-11ed-aae5-0242ac130002', 'd530126f-aac4-11ed-aae5-0242ac130002', 'dc2e57e0-aac4-11ed-aae5-0242ac130002', 'd4e50670-aac4-11ed-aae5-0242ac130002', 'f19d127f-aac4-11ed-aae5-0242ac130002', 'd50f3ea4-aac4-11ed-aae5-0242ac130002', 'd4e0fa65-aac4-11ed-aae5-0242ac130002', 'd49b0b29-aac4-11ed-aae5-0242ac130002', 'd50b5644-aac4-11ed-aae5-0242ac130002', 'd5149d7e-aac4-11ed-aae5-0242ac130002', 'd51449b8-aac4-11ed-aae5-0242ac130002', 'dc22e914-aac4-11ed-aae5-0242ac130002', 'd53f01ad-aac4-11ed-aae5-0242ac130002', 'dd906d58-aac4-11ed-aae5-0242ac130002', 'd8cd42de-aac4-11ed-aae5-0242ac130002', 'd530126f-aac4-11ed-aae5-0242ac130002', 'd4ad81f1-aac4-11ed-aae5-0242ac130002', 'd5149d7e-aac4-11ed-aae5-0242ac130002', 'f305109d-aac4-11ed-aae5-0242ac130002', 'd510545e-aac4-11ed-aae5-0242ac130002', 'd975c956-aac4-11ed-aae5-0242ac130002', 'd4b19f74-aac4-11ed-aae5-0242ac130002', 'd50f0380-aac4-11ed-aae5-0242ac130002', 'd4e0d88e-aac4-11ed-aae5-0242ac130002', 'd760b725-aac4-11ed-aae5-0242ac130002', 'd50b5644-aac4-11ed-aae5-0242ac130002', 'd4e505ed-aac4-11ed-aae5-0242ac130002', 'd8d6f19b-aac4-11ed-aae5-0242ac130002', 'dcb08746-aac4-11ed-aae5-0242ac130002', 'd4e5ca6b-aac4-11ed-aae5-0242ac130002', 'f256bead-aac4-11ed-aae5-0242ac130002', 'f256bead-aac4-11ed-aae5-0242ac130002', 'd468993d-aac4-11ed-aae5-0242ac130002', 'd4e5ca6b-aac4-11ed-aae5-0242ac130002', 'd5c4d509-aac4-11ed-aae5-0242ac130002', 'd55fce49-aac4-11ed-aae5-0242ac130002', 'd530126f-aac4-11ed-aae5-0242ac130002', 'd47e2042-aac4-11ed-aae5-0242ac130002', 'd726ace0-aac4-11ed-aae5-0242ac130002', 'da25fee8-aac4-11ed-aae5-0242ac130002', 'd5145658-aac4-11ed-aae5-0242ac130002', 'd89ab210-aac4-11ed-aae5-0242ac130002', 'f29c64a0-aac4-11ed-aae5-0242ac130002', 'd5c09b69-aac4-11ed-aae5-0242ac130002', 'd4976938-aac4-11ed-aae5-0242ac130002', 'd5149d7e-aac4-11ed-aae5-0242ac130002'], 'PWN_syn_id': ['06341431-n', '10602985-n', '00784342-v', '00020759-r', '02169702-v', '01158872-v', '', '02587532-v', '10582154-n', '00149510-r', '00149510-r', '02134672-v', '02503305-a', '01494310-v', '01125006-a', '03106110-n', '02735688-n', '02134350-v', '00049220-r', '00641460-a', '00748972-v', '15234764-n', '00309945-a', '03365592-n', '01424456-v', '03365592-n', '01885845-v', '', '03692522-n', '05564590-n', '05704266-n', '00644583-v', '09379111-n', '15101854-n', '00061203-r', '02071420-a', '04928903-n', '02455407-v', '15101854-n', '04546855-n', '04105893-n', '00117620-r', '02053941-v', '02818832-n', '15122231-n', '02455407-v', '00153568-r', '01466978-v', '04546855-n', '01572978-v', '03106110-n', '00090651-r', '02103162-v', '03562126-n', '00979870-v', '02180898-v', '02134350-v', '04594218-n', '00113082-r', '02359464-a', '00784342-v', '02134350-v', '01348174-v', '03532342-n', '01391351-a', '09379111-n', '02394183-v', '05149325-n', '03526198-n', '01431112-a', '02118476-v', '00031899-r', '00967129-a', '01044533-v', '01609287-v', '03106110-n', '04105893-n', '00967129-a', '07289831-n', '10100761-n', '02604760-v', '01311103-v', '03526198-n', '00566342-a', '04105893-n', '01282023-v', '', '05635055-n', '01009821-v', '10243137-n', '01712704-v', '15113229-n', '03106110-n', '01020005-v', '02526085-v', '01280349-a', '07358576-n', '00059086-r', '00059086-r', '01343918-a', '07358576-n', '03562126-n', '08403631-n', '03526198-n', '01525666-v', '06341431-n', '01141841-n', '06341249-n', '00345761-v', '00049220-r', '00177127-n', '01013279-a', '04105893-n'], 'bn_syn_id': ['bn:00056181n', 'bn:00071838n', 'bn:00082817v', 'bn:00116326r', 'bn:00089277v', 'bn:00082705v', '', 'bn:00085710v', 'bn:00067423n', 'bn:00114117r', 'bn:00114117r', 'bn:00082693v', 'bn:00108015a', 'bn:00090224v', 'bn:00110887a', 'bn:00022589n', 'bn:00005517n', 'bn:00088105v', 'bn:00114424r', 'bn:00100569a', 'bn:00088841v', 'bn:00055094n', 'bn:00099860a', 'bn:00035304n', 'bn:00083805v', 'bn:00035304n', 'bn:00085991v', '', 'bn:00048187n', 'bn:00042759n', 'bn:00000475n', 'bn:00082596v', 'bn:00037278n', 'bn:00011639n', 'bn:00114165r', 'bn:00110647a', 'bn:00033729n', 'bn:00088428v', 'bn:00011639n', 'bn:00080386n', 'bn:00068233n', 'bn:00114281r', 'bn:00082732v', 'bn:00009472n', 'bn:00077268n', 'bn:00088428v', 'bn:00114671r', 'bn:00082188v', 'bn:00080386n', 'bn:00085108v', 'bn:00022589n', 'bn:00115213r', 'bn:00087253v', 'bn:00045998n', 'bn:00093290v', 'bn:00091505v', 'bn:00088105v', 'bn:00081386n', 'bn:00115016r', 'bn:00111524a', 'bn:00082817v', 'bn:00088105v', 'bn:00090427v', 'bn:00044692n', 'bn:00106058a', 'bn:00037278n', 'bn:00088178v', 'bn:00036822n', 'bn:00044408n', 'bn:00096217a', 'bn:00090661v', 'bn:00116789r', 'bn:00111281a', 'bn:00090707v', 'bn:00092138v', 'bn:00022589n', 'bn:00068233n', 'bn:00111281a', 'bn:00076922n', 'bn:00035672n', 'bn:00083181v', 'bn:00086808v', 'bn:00044408n', 'bn:00096456a', 'bn:00068233n', 'bn:00089423v', '', 'bn:00036493n', 'bn:00093291v', 'bn:00049666n', 'bn:00087107v', 'bn:00061605n', 'bn:00022589n', 'bn:00090740v', 'bn:00082226v', 'bn:00104860a', 'bn:00078008n', 'bn:00116295r', 'bn:00116295r', 'bn:00105276a', 'bn:00078008n', 'bn:00045998n', 'bn:00058568n', 'bn:00044408n', 'bn:00088629v', 'bn:00056181n', 'bn:00002935n', 'bn:00055319n', 'bn:00083340v', 'bn:00114424r', 'bn:00026807n', 'bn:00105740a', 'bn:00068233n'], 'mapping_relation': ['synonymy', 'synonymy', 'synonymy', 'synonymy', 'synonymy', 'synonymy', '', 'hypernymy', 'synonymy', 'hypernymy', 'hypernymy', 'synonymy', 'hypernymy', 'synonymy', 'hypernymy', 'partial synonymy', 'partial synonymy', 'hyponymy', 'synonymy', 'hypernymy', 'synonymy', 'synonymy', 'hypernymy', 'synonymy', 'hypernymy', 'synonymy', 'synonymy', '', 'hyponymy', 'synonymy', 'synonymy', 'hypernymy', 'synonymy', 'synonymy', 'synonymy', 'synonymy', 'partial synonymy', 'hypernymy', 'synonymy', 'synonymy', 'hypernymy', 'hypernymy', 'hypernymy', 'synonymy', 'synonymy', 'hypernymy', 'synonymy', 'hypernymy', 'synonymy', 'hyponymy', 'partial synonymy', 'synonymy', 'hyponymy', 'hypernymy', 'synonymy', 'synonymy', 'hyponymy', 'hypernymy', 'hypernymy', 'hypernymy', 'hypernymy', 'hyponymy', 'hypernymy', 'synonymy', 'synonymy', 'partial synonymy', 'synonymy', 'partial synonymy', 'hypernymy', 'hypernymy', 'synonymy', 'synonymy', 'synonymy', 'hypernymy', 'synonymy', 'partial synonymy', 'hypernymy', 'synonymy', 'hyponymy', 'synonymy', 'synonymy', 'hypernymy', 'hypernymy', 'synonymy', 'hypernymy', 'hypernymy', '', 'hypernymy', 'synonymy', 'partial synonymy', 'hypernymy', 'hypernymy', 'partial synonymy', 'synonymy', 'hypernymy', 'hypernymy', 'hypernymy', 'hypernymy', 'hypernymy', 'hypernymy', 'hypernymy', 'hypernymy', 'hypernymy', 'hypernymy', 'synonymy', 'synonymy', 'partial synonymy', 'synonymy', 'hypernymy', 'synonymy', 'hypernymy', 'synonymy', 'hypernymy']}}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "unique_pos_tags = set()\n",
    "\n",
    "# Loop through each example in the train dataset\n",
    "for example in dataset['train']:\n",
    "    print(example)\n",
    "    # Each example['tokens'] should be a list of token dictionaries\n",
    "    for token in example['tokens']:\n",
    "        # Append the 'pos' field of each token to the set\n",
    "        # unique_pos_tags.add(token['pos'])\n",
    "        # print(token['pos'])\n",
    "        break\n",
    "    break\n",
    "# Print the unique POS tags\n",
    "print(unique_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at sdadas/polish-gpt2-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model_bert = AutoModelForTokenClassification.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"sdadas/polish-gpt2-medium\", add_prefix_space=True)\n",
    "tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
    "model_gpt = AutoModelForTokenClassification.from_pretrained(\"sdadas/polish-gpt2-medium\", pad_token_id=tokenizer_gpt.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_token_classification(examples):\n",
    "    # print(examples['tokens'][1]['orth'])\n",
    "    inputs = []\n",
    "    for example in examples['tokens']:\n",
    "        inputs.append(example['orth'])\n",
    "    tokenized_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=128, is_split_into_words=True)\n",
    "    labels = []\n",
    "    pos = []\n",
    "    for example in examples['tokens']:\n",
    "        pos.append(example['pos'])\n",
    "    # for i, label in enumerate(pos):\n",
    "    #     word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "    #     label_ids = [label[word_id] for word_id in word_ids]\n",
    "    #     labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = pos\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tokenizer_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bert = dataset.map(preprocess_token_classification, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tokenizer_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ab545f6c87480b8f81bc9cef6f38c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_gpt = dataset.map(preprocess_token_classification, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = dataset_bert['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, tokenizer, model, layer=-1):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs,output_hidden_states=True)\n",
    "    # print(outputs)\n",
    "    # x = outputs.hidden_states[-1][:, 0, :].cpu().detach().numpy().squeeze()\n",
    "    x = outputs.hidden_states[layer].cpu().detach().numpy().squeeze()  \n",
    "\n",
    "    return x\n",
    "    # return outputs.last_hidden_state.squeeze(0).detach()  # Embeddings for each token\n",
    "\n",
    "# Example for a sentence\n",
    "sentence = dataset['train'][0]['text']  # Replace with the correct column\n",
    "bert_embeddings = get_embeddings(sentence, tokenizer_bert, model_bert, layer=-1)\n",
    "gpt2_embeddings = get_embeddings(sentence, tokenizer_gpt, model_gpt, layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Anisotropy: 0.7437189999118894\n",
      "GPT-2 Anisotropy: 0.6160065718214576\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "\n",
    "def measure_anisotropy(embeddings):\n",
    "    # Compute cosine similarities for pairs of embeddings\n",
    "    cos_similarities = []\n",
    "    num_samples = 1000  # Adjust for sampling efficiency\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # print(embeddings.size)\n",
    "        idx1, idx2 = torch.randint(0, embeddings.shape[0], (2,))\n",
    "        emb1 = embeddings[idx1]\n",
    "        # print(emb1)\n",
    "        sim = 1 - cosine(embeddings[idx1], embeddings[idx2])\n",
    "        cos_similarities.append(sim)\n",
    "    \n",
    "    return sum(cos_similarities) / len(cos_similarities)\n",
    "\n",
    "bert_anisotropy = measure_anisotropy(bert_embeddings)\n",
    "gpt2_anisotropy = measure_anisotropy(gpt2_embeddings)\n",
    "print(\"BERT Anisotropy:\", bert_anisotropy)\n",
    "print(\"GPT-2 Anisotropy:\", gpt2_anisotropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers_bert = len(model_bert.bert.encoder.layer)\n",
    "num_layers_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers_gpt = len(model_gpt.transformer.h)\n",
    "num_layers_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_anisotropies = []\n",
    "for i in range(num_layers_bert):\n",
    "    bert_embeddings = get_embeddings(sentence, tokenizer_bert, model_bert, layer=i)\n",
    "    anisotropy = measure_anisotropy(bert_embeddings)\n",
    "    bert_anisotropies.append(anisotropy)\n",
    "\n",
    "gpt_anisotropies = []\n",
    "for i in range(num_layers_gpt):\n",
    "    gpt_embeddings = get_embeddings(sentence, tokenizer_gpt, model_gpt, layer=i)\n",
    "    anisotropy = measure_anisotropy(gpt_embeddings)\n",
    "    gpt_anisotropies.append(anisotropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "BERT",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          0.02637671886154652,
          0.2597609275196597,
          0.6082904160855843,
          0.6567974076703671,
          0.6113821335895824,
          0.6313297266424844,
          0.645458798213823,
          0.67148484332492,
          0.6790381762259431,
          0.6842039405633836,
          0.6702018159132408,
          0.7423641379720575
         ]
        },
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "GPT-2",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23
         ],
         "y": [
          0.08929993111969424,
          0.6430113918249692,
          0.6486046974048366,
          0.6467896479434017,
          0.6223932016692348,
          0.5960997920272152,
          0.5666332229190517,
          0.5622089279868411,
          0.5391555607230109,
          0.527005553648602,
          0.5107834650617828,
          0.5052295792828108,
          0.5054898005780611,
          0.5049520739559075,
          0.4970569812762516,
          0.49229880373433627,
          0.4692771235362925,
          0.4724508791462971,
          0.47317889173884103,
          0.4761630698667673,
          0.5078389818871235,
          0.5471731195230718,
          0.6049038622235862,
          0.6172008887869943
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Model"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Anisotropy Comparison: BERT vs GPT-2"
        },
        "xaxis": {
         "title": {
          "text": "Layer Number"
         }
        },
        "yaxis": {
         "title": {
          "text": "Anisotropy Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming bert_anisotropies and gpt_anisotropies are already populated\n",
    "# and num_layers_bert and num_layers_gpt are defined\n",
    "\n",
    "# Create a DataFrame for the plot\n",
    "data = {\n",
    "    'Layer': list(range(num_layers_bert)) + list(range(num_layers_gpt)),\n",
    "    'Anisotropy': bert_anisotropies + gpt_anisotropies,\n",
    "    'Model': ['BERT'] * num_layers_bert + ['GPT-2'] * num_layers_gpt\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the scatter plot with connected lines using plotly.graph_objects\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add BERT trace\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'BERT']['Layer'], \n",
    "                         y=df[df['Model'] == 'BERT']['Anisotropy'],\n",
    "                         mode='markers+lines',  # 'markers+lines' to plot both dots and lines\n",
    "                         name='BERT',\n",
    "                         line=dict(shape='linear', dash='dot')))  # Connect the dots with a linear line\n",
    "\n",
    "# Add GPT-2 trace\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'GPT-2']['Layer'], \n",
    "                         y=df[df['Model'] == 'GPT-2']['Anisotropy'],\n",
    "                         mode='markers+lines',  # 'markers+lines' to plot both dots and lines\n",
    "                         name='GPT-2',\n",
    "                         line=dict(shape='linear', dash='dot')))  # Connect the dots with a linear line\n",
    "\n",
    "# Add labels and title\n",
    "fig.update_layout(\n",
    "    title=\"Anisotropy Comparison: BERT vs GPT-2\",\n",
    "    xaxis_title=\"Layer Number\",\n",
    "    yaxis_title=\"Anisotropy Value\",\n",
    "    legend_title=\"Model\",\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
