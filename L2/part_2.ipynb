{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from peft import AutoPeftModelForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1006a39e0480443fb52eab2d945fcbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac57d374c954fa1b4750a4a0543ff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wsd_polish_datasets.py:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f7fc36f85945e28a659db40e973d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sherlock_text.jsonl:   0%|          | 0.00/2.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a74702337346f4beb7ad6b1e11e8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "skladnica_text.jsonl:   0%|          | 0.00/29.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2888b953d00043cbb7b62e614268579e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikiglex_text.jsonl:   0%|          | 0.00/12.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ea786c4518441383a50d9f4a888ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "emoglex_text.jsonl:   0%|          | 0.00/23.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed16ece8b74451089235871be5d5a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "walenty_text.jsonl:   0%|          | 0.00/50.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ded8ae22e794ca7b8a00e2cb1c51a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "kpwr_text.jsonl:   0%|          | 0.00/57.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda6825f5328409db06aa957634dc928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "kpwr-100_text.jsonl:   0%|          | 0.00/8.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bb506f12c44cb99f17754e7bfb8df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"clarin-knext/wsd_polish_datasets\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'tokens', 'phrases', 'wsd'],\n",
       "        num_rows: 7848\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features['tokens'].feature['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1095f508e84a07925210ff03cfbf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  64%|######4   | 419M/654M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbf8d46abb94a829acd29a297538550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b99400cd92f43c4aab5837c5db694d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409b340c102748c9b38705969522b88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9924c3857d4d3a9d570c000138646c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064a8ddbeec14dbda9b366691612bd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at sdadas/polish-gpt2-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model_bert = AutoModelForTokenClassification.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"sdadas/polish-gpt2-medium\", add_prefix_space=True)\n",
    "tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
    "model_gpt = AutoModelForTokenClassification.from_pretrained(\"sdadas/polish-gpt2-medium\", pad_token_id=tokenizer_gpt.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, tokenizer, model, layer=-1):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs,output_hidden_states=True)\n",
    "    # print(outputs)\n",
    "    # x = outputs.hidden_states[-1][:, 0, :].cpu().detach().numpy().squeeze()\n",
    "    x = outputs.hidden_states[layer].cpu().detach().numpy().squeeze()  \n",
    "\n",
    "    return x\n",
    "    # return outputs.last_hidden_state.squeeze(0).detach()  # Embeddings for each token\n",
    "\n",
    "# Example for a sentence\n",
    "sentence = dataset['train'][0]['text']  # Replace with the correct column\n",
    "bert_embeddings = get_embeddings(sentence, tokenizer_bert, model_bert, layer=-1)\n",
    "gpt2_embeddings = get_embeddings(sentence, tokenizer_gpt, model_gpt, layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Anisotropy: 0.7217383639671769\n",
      "GPT-2 Anisotropy: 0.24481897037399522\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "\n",
    "def measure_anisotropy(embeddings):\n",
    "    # Compute cosine similarities for pairs of embeddings\n",
    "    cos_similarities = []\n",
    "    num_samples = 1000  # Adjust for sampling efficiency\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # print(embeddings.size)\n",
    "        idx1, idx2 = torch.randint(0, embeddings.shape[0], (2,))\n",
    "        emb1 = embeddings[idx1]\n",
    "        # print(emb1)\n",
    "        sim = 1 - cosine(embeddings[idx1], embeddings[idx2])\n",
    "        cos_similarities.append(sim)\n",
    "    \n",
    "    return sum(cos_similarities) / len(cos_similarities)\n",
    "\n",
    "bert_anisotropy = measure_anisotropy(bert_embeddings)\n",
    "gpt2_anisotropy = measure_anisotropy(gpt2_embeddings)\n",
    "print(\"BERT Anisotropy:\", bert_anisotropy)\n",
    "print(\"GPT-2 Anisotropy:\", gpt2_anisotropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers_bert = len(model_bert.bert.encoder.layer)\n",
    "num_layers_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers_gpt = len(model_gpt.transformer.h)\n",
    "num_layers_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_anisotropies = []\n",
    "for i in range(num_layers_bert):\n",
    "    bert_embeddings = get_embeddings(sentence, tokenizer_bert, model_bert, layer=i)\n",
    "    anisotropy = measure_anisotropy(bert_embeddings)\n",
    "    bert_anisotropies.append(anisotropy)\n",
    "\n",
    "gpt_anisotropies = []\n",
    "for i in range(num_layers_gpt):\n",
    "    gpt_embeddings = get_embeddings(sentence, tokenizer_gpt, model_gpt, layer=i)\n",
    "    anisotropy = measure_anisotropy(gpt_embeddings)\n",
    "    gpt_anisotropies.append(anisotropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "BERT",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          0.0205608218880867,
          0.2595294213844444,
          0.6076908806570862,
          0.6521327964110023,
          0.6092404026215908,
          0.6359398872497144,
          0.6483083953157461,
          0.6775227349447741,
          0.6802891619808161,
          0.6836489426528616,
          0.6611278981255139,
          0.7381863791144044
         ]
        },
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "GPT-2",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23
         ],
         "y": [
          0.10087204309724279,
          0.6437892545962296,
          0.6463636582287761,
          0.6393947338151362,
          0.6199710126573166,
          0.5894037426325993,
          0.5738295170346137,
          0.5636085158982005,
          0.5419752891465496,
          0.5194731405574881,
          0.511031866131547,
          0.5007950959017279,
          0.5062111242406881,
          0.5027692239019801,
          0.503526692971252,
          0.49486901433269176,
          0.4782985061592762,
          0.4669401223046102,
          0.47427612679418674,
          0.48391306807593293,
          0.5079819569241536,
          0.5491258891400314,
          0.6036614340297446,
          0.6189319372737047
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Model"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Anisotropy Comparison: BERT vs GPT-2"
        },
        "xaxis": {
         "title": {
          "text": "Layer Number"
         }
        },
        "yaxis": {
         "title": {
          "text": "Anisotropy Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Layer': list(range(num_layers_bert)) + list(range(num_layers_gpt)),\n",
    "    'Anisotropy': bert_anisotropies + gpt_anisotropies,\n",
    "    'Model': ['BERT'] * num_layers_bert + ['GPT-2'] * num_layers_gpt\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'BERT']['Layer'], \n",
    "                         y=df[df['Model'] == 'BERT']['Anisotropy'],\n",
    "                         mode='markers+lines',\n",
    "                         name='BERT',\n",
    "                         line=dict(shape='linear', dash='dot')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'GPT-2']['Layer'], \n",
    "                         y=df[df['Model'] == 'GPT-2']['Anisotropy'],\n",
    "                         mode='markers+lines',\n",
    "                         name='GPT-2',\n",
    "                         line=dict(shape='linear', dash='dot')))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Anisotropy Comparison: BERT vs GPT-2\",\n",
    "    xaxis_title=\"Layer Number\",\n",
    "    yaxis_title=\"Anisotropy Value\",\n",
    "    legend_title=\"Model\",\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataset['train'][:10]['text']  # Replace with the correct column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = \" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1781 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Context-Specificity: 0.8382445823264426\n",
      "GPT-2 Context-Specificity: 0.24329707611341647\n"
     ]
    }
   ],
   "source": [
    "def context_specificity(token, dataset, tokenizer, model, layer=-1):\n",
    "    embeddings = []\n",
    "    texts = dataset['text']\n",
    "    for example in texts:\n",
    "        try:\n",
    "            inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs, output_hidden_states=True).hidden_states[layer].squeeze(0).detach()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Get index of token in the text\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        if token_id in inputs['input_ids']:\n",
    "            token_index = (inputs['input_ids'] == token_id).nonzero(as_tuple=True)[1]\n",
    "            embeddings.append(outputs[token_index].mean(0))  # Averaging over token occurrences\n",
    "\n",
    "    # Compute average cosine similarity between each pair of embeddings\n",
    "    cos_similarities = []\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            sim = 1 - cosine(embeddings[i], embeddings[j])\n",
    "            cos_similarities.append(sim)\n",
    "\n",
    "    return sum(cos_similarities) / len(cos_similarities) if cos_similarities else None\n",
    "\n",
    "# Example usage\n",
    "bert_context_specificity = context_specificity(\"nie\", dataset['train'][:100], tokenizer_bert, model_bert)\n",
    "gpt2_context_specificity = context_specificity(\"nie\", dataset['train'][:100], tokenizer_gpt, model_gpt)\n",
    "\n",
    "print(\"BERT Context-Specificity:\", bert_context_specificity)\n",
    "print(\"GPT-2 Context-Specificity:\", gpt2_context_specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_context = []\n",
    "for i in range(num_layers_bert):\n",
    "    context = context_specificity(\"nie\", dataset['train'][:100], tokenizer_bert, model_bert, layer=i)\n",
    "    bert_context.append(context)\n",
    "\n",
    "gpt_context = []\n",
    "for i in range(num_layers_gpt):\n",
    "    context = context_specificity(\"nie\", dataset['train'][:100], tokenizer_gpt, model_gpt, layer=i)\n",
    "    gpt_context.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "BERT",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          0.8892924954636118,
          0.8326891540447701,
          0.7852610194068175,
          0.7479331550531031,
          0.713739586739526,
          0.6857255016293125,
          0.667430715394179,
          0.6468009397841309,
          0.6921794504624222,
          0.7174691332389604,
          0.706589484387581,
          0.7418768852097439
         ]
        },
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "GPT-2",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23
         ],
         "y": [
          0.9508724593831297,
          0.9264606144587894,
          0.899361592500509,
          0.8403586155920163,
          0.7562098548860117,
          0.6949593460081518,
          0.6628160918536098,
          0.639305294722998,
          0.6246413963503724,
          0.6035428556090265,
          0.5845363248099831,
          0.5672975548153812,
          0.5702460898938737,
          0.5760687966171792,
          0.5777400429593116,
          0.5737599671079956,
          0.562389093012651,
          0.5556394648321298,
          0.5611628636802414,
          0.5675801260266029,
          0.587584449882135,
          0.6121093601281207,
          0.6497593039039014,
          0.6594467477012638
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Model"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Context-Specificity Comparison: BERT vs GPT-2"
        },
        "xaxis": {
         "title": {
          "text": "Layer Number"
         }
        },
        "yaxis": {
         "title": {
          "text": "Context-Specificity Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    'Layer': list(range(num_layers_bert)) + list(range(num_layers_gpt)),\n",
    "    'Context': bert_context + gpt_context,\n",
    "    'Model': ['BERT'] * num_layers_bert + ['GPT-2'] * num_layers_gpt\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'BERT']['Layer'], \n",
    "                         y=df[df['Model'] == 'BERT']['Context'],\n",
    "                         mode='markers+lines',\n",
    "                         name='BERT',\n",
    "                         line=dict(shape='linear', dash='dot')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'GPT-2']['Layer'], \n",
    "                         y=df[df['Model'] == 'GPT-2']['Context'],\n",
    "                         mode='markers+lines',\n",
    "                         name='GPT-2',\n",
    "                         line=dict(shape='linear', dash='dot')))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Context-Specificity Comparison: BERT vs GPT-2\",\n",
    "    xaxis_title=\"Layer Index\",\n",
    "    yaxis_title=\"Context-Specificity Value\",\n",
    "    legend_title=\"Model\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm, trange\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALNUM_CHARSET = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "\n",
    "def convert_to_tokens(indices, tokenizer, extended=False, extra_values_pos=None, strip=True):\n",
    "    if extended:\n",
    "        res = [tokenizer.convert_ids_to_tokens([idx])[0] if idx < len(tokenizer) else \n",
    "               (f\"[pos{idx-len(tokenizer)}]\" if idx < extra_values_pos else f\"[val{idx-extra_values_pos}]\") \n",
    "               for idx in indices]\n",
    "    else:\n",
    "        res = tokenizer.convert_ids_to_tokens(indices)\n",
    "    if strip:\n",
    "        res = list(map(lambda x: x[1:] if x[0] == 'Ġ' else \"#\" + x, res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_tokens(v, k=100, tokenizer=None, only_alnum=False, only_ascii=True, with_values=False, \n",
    "               exclude_brackets=False, extended=True, extra_values=None, only_from_list=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    v = deepcopy(v)\n",
    "    ignored_indices = []\n",
    "    if only_ascii:\n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not val.strip('Ġ▁').isascii()])\n",
    "    if only_alnum: \n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not (set(val.strip('Ġ▁[] ')) <= ALNUM_CHARSET)])\n",
    "    if only_from_list:\n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if val.strip('Ġ▁ ').lower() not in only_from_list])\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "        \n",
    "    ignored_indices = list(set(ignored_indices))\n",
    "    v[ignored_indices] = -np.inf\n",
    "    extra_values_pos = len(v)\n",
    "    if extra_values is not None:\n",
    "        v = torch.cat([v, extra_values])\n",
    "    values, indices = torch.topk(v, k=k)\n",
    "    res = convert_to_tokens(indices, tokenizer, extended=extended, extra_values_pos=extra_values_pos)\n",
    "    if with_values:\n",
    "        res = list(zip(res, values.cpu().numpy()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Weights GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"sdadas/polish-gpt2-medium\",\n",
       "  \"activation_function\": \"gelu_fast\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 1024,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": 4096,\n",
       "  \"n_layer\": 24,\n",
       "  \"n_positions\": 2048,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"tokenizer_class\": \"GPT2TokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 51200\n",
       "}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer_gpt = AutoTokenizer.from_pretrained(\n",
    "#     \"sdadas/polish-gpt2-medium\", add_prefix_space=True\n",
    "# )\n",
    "# tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
    "# model_gpt = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"sdadas/polish-gpt2-medium\", pad_token_id=tokenizer_gpt.pad_token_id\n",
    "# )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"sdadas/polish-gpt2-medium\")\n",
    "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(\"sdadas/polish-gpt2-medium\")\n",
    "emb = model.get_output_embeddings().weight.data.T.detach()\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_layers = model.config.n_layer\n",
    "num_heads = model.config.n_head\n",
    "hidden_dim = model.config.n_embd\n",
    "head_size = hidden_dim // num_heads\n",
    "\n",
    "K = torch.cat(\n",
    "    [\n",
    "        model.get_parameter(f\"transformer.h.{j}.mlp.c_fc.weight\").T\n",
    "        for j in range(num_layers)\n",
    "    ]\n",
    ").detach()\n",
    "V = torch.cat(\n",
    "    [\n",
    "        model.get_parameter(f\"transformer.h.{j}.mlp.c_proj.weight\")\n",
    "        for j in range(num_layers)\n",
    "    ]\n",
    ").detach()\n",
    "\n",
    "W_Q, W_K, W_V = (\n",
    "    torch.cat(\n",
    "        [\n",
    "            model.get_parameter(f\"transformer.h.{j}.attn.c_attn.weight\")\n",
    "            for j in range(num_layers)\n",
    "        ]\n",
    "    )\n",
    "    .detach()\n",
    "    .chunk(3, dim=-1)\n",
    ")\n",
    "W_O = torch.cat(\n",
    "    [\n",
    "        model.get_parameter(f\"transformer.h.{j}.attn.c_proj.weight\")\n",
    "        for j in range(num_layers)\n",
    "    ]\n",
    ").detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_heads = K.reshape(num_layers, -1, hidden_dim)\n",
    "V_heads = V.reshape(num_layers, -1, hidden_dim)\n",
    "d_int = K_heads.shape[1]\n",
    "\n",
    "W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(\n",
    "    0, 2, 1, 3\n",
    ")\n",
    "W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(\n",
    "    0, 2, 1, 3\n",
    ")\n",
    "W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(\n",
    "    0, 2, 1, 3\n",
    ")\n",
    "W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_inv = emb.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1, i2 = 23, 907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 907\n"
     ]
    }
   ],
   "source": [
    "print(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K          V              -K          -V\n",
      "---------  -------------  ----------  ---------\n",
      "przycu     kody           dotychczas  #bot\n",
      "zalog      #gory          rodzi       #lot\n",
      "wylegi     #ei            do          #ush\n",
      "#walifik   #zmy           przebie     #dzista\n",
      "przesp     Apo            gatunku     #remont\n",
      "pochowany  apokali        przez       #lee\n",
      "#cket      #128           #ja         #wan\n",
      "#ppe       ludy           rodzin      #ette\n",
      "wep        #ords          zrazu       #up\n",
      "sfinans    Cezary         lokalnie    #bul\n",
      "#iss       archa          porywa      #puszczam\n",
      "#CS        przy           two         spu\n",
      "#gny       akcy           jeszcze     zamyka\n",
      "#zwol      Homo           #jaw        odstawi\n",
      "#-).       litera         na          #mont\n",
      "lock       #pka           wymaga      #beki\n",
      "skonfisk   Cezar          ty          #laks\n",
      "#post      Benedykt       Drze        tap\n",
      "postoju    narodem        pod         poby\n",
      "erek       polityki       Nie         posto\n",
      "#ionu      symbolu        rzuca       #wana\n",
      "#ppo       #lachet        ludzkim     #dzana\n",
      "unierucho  publicznego    uczucia     #load\n",
      "#ott       #rzami         rze         #owol\n",
      "zbombar    anie           tak         zatk\n",
      "#FF        uniwersalny    da          opuszcza\n",
      "#prem      artystycznego  ludzkie     znajd\n",
      "#klaski    ustawowego     natury      #dzany\n",
      "#gs        werb           #obie       #elli\n",
      "zatk       wzroku         pewnego     #bek\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tabulate(\n",
    "        [\n",
    "            *zip(\n",
    "                top_tokens(\n",
    "                    (K_heads[i1, i2]) @ emb,\n",
    "                    k=30,\n",
    "                    only_from_list=tokens_list,\n",
    "                    only_alnum=False,\n",
    "                ),\n",
    "                top_tokens(\n",
    "                    (V_heads[i1, i2]) @ emb,\n",
    "                    k=30,\n",
    "                    only_from_list=tokens_list,\n",
    "                    only_alnum=False,\n",
    "                ),\n",
    "                top_tokens((-K_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    "                top_tokens((-V_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    "            )\n",
    "        ],\n",
    "        headers=[\"K\", \"V\", \"-K\", \"-V\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_topk(mat, min_k=500, max_k=250_000, th0=10, max_iters=10, verbose=False):\n",
    "    _get_actual_k = lambda th, th_max: torch.nonzero((mat > th) & (mat < th_max)).shape[\n",
    "        0\n",
    "    ]\n",
    "    th_max = np.inf\n",
    "    left, right = 0, th0\n",
    "    while True:\n",
    "        actual_k = _get_actual_k(right, th_max)\n",
    "        if verbose:\n",
    "            print(f\"one more iteration. {actual_k}\")\n",
    "        if actual_k <= max_k:\n",
    "            break\n",
    "        left, right = right, right * 2\n",
    "    if min_k <= actual_k <= max_k:\n",
    "        th = right\n",
    "    else:\n",
    "        for _ in range(max_iters):\n",
    "            mid = (left + right) / 2\n",
    "            actual_k = _get_actual_k(mid, th_max)\n",
    "            if verbose:\n",
    "                print(f\"one more iteration. {actual_k}\")\n",
    "            if min_k <= actual_k <= max_k:\n",
    "                break\n",
    "            if actual_k > max_k:\n",
    "                left = mid\n",
    "            else:\n",
    "                right = mid\n",
    "        th = mid\n",
    "    return torch.nonzero((mat > th) & (mat < th_max)).tolist()\n",
    "\n",
    "\n",
    "def get_top_entries(\n",
    "    tmp,\n",
    "    all_high_pos,\n",
    "    only_ascii=False,\n",
    "    only_alnum=False,\n",
    "    exclude_same=False,\n",
    "    exclude_fuzzy=False,\n",
    "    tokens_list=None,\n",
    "):\n",
    "    remaining_pos = all_high_pos\n",
    "    if only_ascii:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: (\n",
    "                    tokenizer.decode(x[0]).strip(\"Ġ▁\").isascii()\n",
    "                    and tokenizer.decode(x[1]).strip(\"Ġ▁\").isascii()\n",
    "                ),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "    if only_alnum:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: (\n",
    "                    tokenizer.decode(x[0]).strip(\"Ġ▁ \").isalnum()\n",
    "                    and tokenizer.decode(x[1]).strip(\"Ġ▁ \").isalnum()\n",
    "                ),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "    if exclude_same:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: tokenizer.decode(x[0]).lower().strip()\n",
    "                != tokenizer.decode(x[1]).lower().strip(),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "    if exclude_fuzzy:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: not _fuzzy_eq(\n",
    "                    tokenizer.decode(x[0]).lower().strip(),\n",
    "                    tokenizer.decode(x[1]).lower().strip(),\n",
    "                ),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "    if tokens_list:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: (\n",
    "                    (tokenizer.decode(x[0]).strip(\"Ġ▁\").lower().strip() in tokens_list)\n",
    "                    and (\n",
    "                        tokenizer.decode(x[1]).strip(\"Ġ▁\").lower().strip()\n",
    "                        in tokens_list\n",
    "                    )\n",
    "                ),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    pos_val = tmp[[*zip(*remaining_pos)]]\n",
    "    good_cells = [\n",
    "        *map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos)\n",
    "    ]\n",
    "    good_tokens = list(map(lambda x: Counter(x).most_common(), zip(*good_cells)))\n",
    "    remaining_pos_best = np.array(remaining_pos)[\n",
    "        torch.argsort(pos_val if reverse_list else -pos_val)[:50]\n",
    "    ]\n",
    "    good_cells_best = [\n",
    "        *map(\n",
    "            lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])),\n",
    "            remaining_pos_best,\n",
    "        )\n",
    "    ]\n",
    "    # good_cells[:100]\n",
    "    # list(zip(good_tokens[0], good_tokens[1]))\n",
    "    return good_cells_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wvo Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 7)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = np.random.randint(num_layers), np.random.randint(num_heads)\n",
    "i1, i2 = 21, 7\n",
    "i1, i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V_tmp, W_O_tmp = W_V_heads[i1, i2, :], W_O_heads[i1, i2]\n",
    "tmp = emb_inv @ (W_V_tmp @ W_O_tmp) @ emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 10\n",
      "one more iteration. 5182\n"
     ]
    }
   ],
   "source": [
    "all_high_pos = approx_topk(\n",
    "    tmp, th0=1, verbose=True\n",
    ")  # torch.nonzero((tmp > th) & (tmp < th_max)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_same = False\n",
    "reverse_list = False\n",
    "only_ascii = True\n",
    "only_alnum = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ranem', ' nad'),\n",
       " ('ornie', ' przez'),\n",
       " ('datek', ' nad'),\n",
       " ('arl', ' przed'),\n",
       " (' pewno', ' na'),\n",
       " ('spodziewanie', ' nad'),\n",
       " (' razu', ' od'),\n",
       " ('miernie', ' nad'),\n",
       " (' okazji', ' przy'),\n",
       " (' wsk', ' na'),\n",
       " (' czele', ' na'),\n",
       " ('orne', ' przez'),\n",
       " ('godziny', ' nad'),\n",
       " ('sione', ' przed'),\n",
       " (' ranem', 'nad'),\n",
       " ('natural', ' nad'),\n",
       " ('przewo', ' nad'),\n",
       " (' Duna', ' nad'),\n",
       " ('ktory', ' do'),\n",
       " ('miar', ' nad'),\n",
       " (' dobra', ' dla'),\n",
       " (' Jezi', ' nad'),\n",
       " ('spodzie', ' nad'),\n",
       " (' niedawna', ' do'),\n",
       " ('pisie', ' pod'),\n",
       " (' wygody', ' dla'),\n",
       " ('arcie', ' przed'),\n",
       " (' sumie', ' w'),\n",
       " ('miernie', 'nad'),\n",
       " ('tek', ' pod'),\n",
       " ('wcze', ' przed'),\n",
       " ('mier', ' nad'),\n",
       " ('hala', ' pod'),\n",
       " ('ornie', ' przeze'),\n",
       " (' uboczu', ' na'),\n",
       " (' podstawie', ' na'),\n",
       " (' ranem', 'Nad'),\n",
       " ('czesne', ' do'),\n",
       " ('granicznych', ' nad'),\n",
       " ('wiska', ' przez'),\n",
       " (' barkach', ' na'),\n",
       " ('ornie', 'przez'),\n",
       " (' odmiany', ' dla'),\n",
       " ('niego', ' przed'),\n",
       " (' plecami', ' za'),\n",
       " (' dobi', ' na'),\n",
       " ('przewodni', ' nad'),\n",
       " (' koniec', ' pod'),\n",
       " (' razie', ' na'),\n",
       " (' potrzeby', ' na')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_entries(\n",
    "    tmp,\n",
    "    all_high_pos,\n",
    "    only_ascii=only_ascii,\n",
    "    only_alnum=only_alnum,\n",
    "    exclude_same=exclude_same,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 1738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('was', '-'),\n",
       " ('was', '--'),\n",
       " ('lla', '-'),\n",
       " ('heim', '-'),\n",
       " ('lle', '-'),\n",
       " ('gonie', '-'),\n",
       " ('lla', '--'),\n",
       " ('ye', '-'),\n",
       " ('osobowej', '-'),\n",
       " ('go', '-'),\n",
       " ('has', '-'),\n",
       " ('zym', '-'),\n",
       " ('tino', '-'),\n",
       " ('kowiec', '-'),\n",
       " (' Stanu', '-'),\n",
       " ('zji', '-'),\n",
       " ('lowie', '-'),\n",
       " ('procent', '-'),\n",
       " ('lle', '-.'),\n",
       " ('stanu', '-'),\n",
       " ('lle', '--'),\n",
       " ('123', '-'),\n",
       " ('gos', '-'),\n",
       " ('head', '-'),\n",
       " ('fil', '--'),\n",
       " ('lla', '-.'),\n",
       " ('kowca', '-'),\n",
       " ('lah', '-'),\n",
       " ('lah', '--'),\n",
       " ('was', '\"-'),\n",
       " ('fil', '-'),\n",
       " ('krotnie', '-'),\n",
       " ('dno', '-'),\n",
       " ('top', '-'),\n",
       " ('lla', '->'),\n",
       " ('sbur', '-'),\n",
       " ('czycy', '-'),\n",
       " ('dzkiego', '-'),\n",
       " ('pii', '-'),\n",
       " ('bak', '-'),\n",
       " ('lowie', '--'),\n",
       " ('lski', '-'),\n",
       " ('lit', '-'),\n",
       " ('has', '--'),\n",
       " ('ben', '-'),\n",
       " ('lla', '-)'),\n",
       " ('ls', '-'),\n",
       " ('tino', '--'),\n",
       " ('lli', '-'),\n",
       " ('zji', '--')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 18, 2\n",
    "i1, i2\n",
    "W_V_tmp, W_O_tmp = W_V_heads[i1, i2, :], W_O_heads[i1, i2]\n",
    "tmp = emb_inv @ (W_V_tmp @ W_O_tmp) @ emb\n",
    "all_high_pos = approx_topk(\n",
    "    tmp, th0=1, verbose=True\n",
    ")  # torch.nonzero((tmp > th) & (tmp < th_max)).tolist()\n",
    "exclude_same = False\n",
    "reverse_list = False\n",
    "only_ascii = True\n",
    "only_alnum = False\n",
    "get_top_entries(\n",
    "    tmp,\n",
    "    all_high_pos,\n",
    "    only_ascii=only_ascii,\n",
    "    only_alnum=only_alnum,\n",
    "    exclude_same=exclude_same,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_gpt = AutoTokenizer.from_pretrained(\n",
    "#     \"sdadas/polish-gpt2-medium\", add_prefix_space=True\n",
    "# )\n",
    "# tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
    "# model_gpt = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"sdadas/polish-gpt2-medium\", pad_token_id=tokenizer_gpt.pad_token_id\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "emb = model.get_output_embeddings().weight.data.T.detach()\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "hidden_dim = model.config.hidden_size\n",
    "head_size = hidden_dim // num_heads\n",
    "\n",
    "K = torch.cat(\n",
    "    [\n",
    "        model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.key.weight\").T\n",
    "        for j in range(num_layers)\n",
    "    ]\n",
    ").detach()\n",
    "V = torch.cat(\n",
    "    [\n",
    "        model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.value.weight\")\n",
    "        for j in range(num_layers)\n",
    "    ]\n",
    ").detach()\n",
    "\n",
    "W_Q, W_K, W_V = (\n",
    "    torch.cat(\n",
    "        [\n",
    "            model.get_parameter(f\"bert.encoder.layer.{j}.attention.self.query.weight\")\n",
    "            for j in range(num_layers)\n",
    "        ]\n",
    "    )\n",
    "    .detach()\n",
    "    .chunk(3, dim=-1)\n",
    ")\n",
    "W_O = torch.cat(\n",
    "    [\n",
    "        model.get_parameter(f\"bert.encoder.layer.{j}.attention.output.dense.weight\")\n",
    "        for j in range(num_layers)\n",
    "    ]\n",
    ").detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9216, 768])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- fix parameter projection for bert base model\n",
    "- generate overall lists for both models\n",
    "- compare the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[12, 768, 12, 64]' is invalid for input of size 2359296",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m V_heads \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mreshape(num_layers, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_dim)\n\u001b[1;32m      6\u001b[0m d_int \u001b[38;5;241m=\u001b[39m K_heads\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m W_Q_heads \u001b[38;5;241m=\u001b[39m \u001b[43mW_Q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m W_K_heads \u001b[38;5;241m=\u001b[39m W_K\u001b[38;5;241m.\u001b[39mreshape(num_layers, hidden_dim, num_heads, head_size)\u001b[38;5;241m.\u001b[39mpermute(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m W_V_heads \u001b[38;5;241m=\u001b[39m W_V\u001b[38;5;241m.\u001b[39mreshape(num_layers, hidden_dim, num_heads, head_size)\u001b[38;5;241m.\u001b[39mpermute(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[12, 768, 12, 64]' is invalid for input of size 2359296"
     ]
    }
   ],
   "source": [
    "assert K.shape[1] == hidden_dim, \"K dimensions do not match hidden_dim\"\n",
    "assert V.shape[1] == hidden_dim, \"V dimensions do not match hidden_dim\"\n",
    "\n",
    "K_heads = K.reshape(num_layers, -1, hidden_dim)\n",
    "V_heads = V.reshape(num_layers, -1, hidden_dim)\n",
    "d_int = K_heads.shape[1]\n",
    "\n",
    "W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(\n",
    "    0, 2, 1, 3\n",
    ")\n",
    "W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(\n",
    "    0, 2, 1, 3\n",
    ")\n",
    "W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(\n",
    "    0, 2, 1, 3\n",
    ")\n",
    "W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
