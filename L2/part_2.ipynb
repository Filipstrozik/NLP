{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from peft import AutoPeftModelForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"clarin-knext/wsd_polish_datasets\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'tokens', 'phrases', 'wsd'],\n",
       "        num_rows: 7848\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features['tokens'].feature['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at sdadas/polish-gpt2-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model_bert = AutoModelForTokenClassification.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"sdadas/polish-gpt2-medium\", add_prefix_space=True)\n",
    "tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
    "model_gpt = AutoModelForTokenClassification.from_pretrained(\"sdadas/polish-gpt2-medium\", pad_token_id=tokenizer_gpt.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, tokenizer, model, layer=-1):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs,output_hidden_states=True)\n",
    "    # print(outputs)\n",
    "    # x = outputs.hidden_states[-1][:, 0, :].cpu().detach().numpy().squeeze()\n",
    "    x = outputs.hidden_states[layer].cpu().detach().numpy().squeeze()  \n",
    "\n",
    "    return x\n",
    "    # return outputs.last_hidden_state.squeeze(0).detach()  # Embeddings for each token\n",
    "\n",
    "# Example for a sentence\n",
    "sentence = dataset['train'][0]['text']  # Replace with the correct column\n",
    "bert_embeddings = get_embeddings(sentence, tokenizer_bert, model_bert, layer=-1)\n",
    "gpt2_embeddings = get_embeddings(sentence, tokenizer_gpt, model_gpt, layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Anisotropy: 0.7162601851025178\n",
      "GPT-2 Anisotropy: 0.2463051208457339\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "\n",
    "def measure_anisotropy(embeddings):\n",
    "    # Compute cosine similarities for pairs of embeddings\n",
    "    cos_similarities = []\n",
    "    num_samples = 1000  # Adjust for sampling efficiency\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # print(embeddings.size)\n",
    "        idx1, idx2 = torch.randint(0, embeddings.shape[0], (2,))\n",
    "        emb1 = embeddings[idx1]\n",
    "        # print(emb1)\n",
    "        sim = 1 - cosine(embeddings[idx1], embeddings[idx2])\n",
    "        cos_similarities.append(sim)\n",
    "    \n",
    "    return sum(cos_similarities) / len(cos_similarities)\n",
    "\n",
    "bert_anisotropy = measure_anisotropy(bert_embeddings)\n",
    "gpt2_anisotropy = measure_anisotropy(gpt2_embeddings)\n",
    "print(\"BERT Anisotropy:\", bert_anisotropy)\n",
    "print(\"GPT-2 Anisotropy:\", gpt2_anisotropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers_bert = len(model_bert.bert.encoder.layer)\n",
    "num_layers_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers_gpt = len(model_gpt.transformer.h)\n",
    "num_layers_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_anisotropies = []\n",
    "for i in range(num_layers_bert):\n",
    "    bert_embeddings = get_embeddings(sentence, tokenizer_bert, model_bert, layer=i)\n",
    "    anisotropy = measure_anisotropy(bert_embeddings)\n",
    "    bert_anisotropies.append(anisotropy)\n",
    "\n",
    "gpt_anisotropies = []\n",
    "for i in range(num_layers_gpt):\n",
    "    gpt_embeddings = get_embeddings(sentence, tokenizer_gpt, model_gpt, layer=i)\n",
    "    anisotropy = measure_anisotropy(gpt_embeddings)\n",
    "    gpt_anisotropies.append(anisotropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "BERT",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          0.0170143673607679,
          0.2563530253091715,
          0.6073481479936381,
          0.655034073281824,
          0.6103743833873781,
          0.6271750058945873,
          0.6464031666917409,
          0.674952092761761,
          0.683052805031502,
          0.6805333376949378,
          0.67317537975845,
          0.7441869751337593
         ]
        },
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "GPT-2",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23
         ],
         "y": [
          0.08205306449239548,
          0.6491795902798742,
          0.6412579459437261,
          0.6435318640646928,
          0.6222865512676565,
          0.5918115050026114,
          0.5738645005675368,
          0.5566018977245804,
          0.5450478974468629,
          0.5197857632545975,
          0.5123800056928198,
          0.5071513444108899,
          0.5056553502201133,
          0.5064934055317619,
          0.5015379211235742,
          0.4925726639677024,
          0.47728989210259615,
          0.46936649761860033,
          0.47802407632088384,
          0.48278620072649403,
          0.5092724417311876,
          0.5529611923689355,
          0.6081264789432922,
          0.6155811703272034
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Model"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Anisotropy Comparison: BERT vs GPT-2"
        },
        "xaxis": {
         "title": {
          "text": "Layer Number"
         }
        },
        "yaxis": {
         "title": {
          "text": "Anisotropy Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Layer': list(range(num_layers_bert)) + list(range(num_layers_gpt)),\n",
    "    'Anisotropy': bert_anisotropies + gpt_anisotropies,\n",
    "    'Model': ['BERT'] * num_layers_bert + ['GPT-2'] * num_layers_gpt\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'BERT']['Layer'], \n",
    "                         y=df[df['Model'] == 'BERT']['Anisotropy'],\n",
    "                         mode='markers+lines',\n",
    "                         name='BERT',\n",
    "                         line=dict(shape='linear', dash='dot')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'GPT-2']['Layer'], \n",
    "                         y=df[df['Model'] == 'GPT-2']['Anisotropy'],\n",
    "                         mode='markers+lines',\n",
    "                         name='GPT-2',\n",
    "                         line=dict(shape='linear', dash='dot')))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Anisotropy Comparison: BERT vs GPT-2\",\n",
    "    xaxis_title=\"Layer Number\",\n",
    "    yaxis_title=\"Anisotropy Value\",\n",
    "    legend_title=\"Model\",\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataset['train'][:10]['text']  # Replace with the correct column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = \" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1781 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Context-Specificity: 0.8382445823264426\n",
      "GPT-2 Context-Specificity: 0.24329707611341647\n"
     ]
    }
   ],
   "source": [
    "def context_specificity(token, dataset, tokenizer, model, layer=-1):\n",
    "    embeddings = []\n",
    "    texts = dataset['text']\n",
    "    for example in texts:\n",
    "        try:\n",
    "            inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs, output_hidden_states=True).hidden_states[layer].squeeze(0).detach()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Get index of token in the text\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        if token_id in inputs['input_ids']:\n",
    "            token_index = (inputs['input_ids'] == token_id).nonzero(as_tuple=True)[1]\n",
    "            embeddings.append(outputs[token_index].mean(0))  # Averaging over token occurrences\n",
    "\n",
    "    # Compute average cosine similarity between each pair of embeddings\n",
    "    cos_similarities = []\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            sim = 1 - cosine(embeddings[i], embeddings[j])\n",
    "            cos_similarities.append(sim)\n",
    "\n",
    "    return sum(cos_similarities) / len(cos_similarities) if cos_similarities else None\n",
    "\n",
    "# Example usage\n",
    "bert_context_specificity = context_specificity(\"nie\", dataset['train'][:100], tokenizer_bert, model_bert)\n",
    "gpt2_context_specificity = context_specificity(\"nie\", dataset['train'][:100], tokenizer_gpt, model_gpt)\n",
    "\n",
    "print(\"BERT Context-Specificity:\", bert_context_specificity)\n",
    "print(\"GPT-2 Context-Specificity:\", gpt2_context_specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_context = []\n",
    "for i in range(num_layers_bert):\n",
    "    context = context_specificity(\"nie\", dataset['train'][:100], tokenizer_bert, model_bert, layer=i)\n",
    "    bert_context.append(context)\n",
    "\n",
    "gpt_context = []\n",
    "for i in range(num_layers_gpt):\n",
    "    context = context_specificity(\"nie\", dataset['train'][:100], tokenizer_gpt, model_gpt, layer=i)\n",
    "    gpt_context.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "BERT",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          0.8892924954636118,
          0.8326891540447701,
          0.7852610194068175,
          0.7479331550531031,
          0.713739586739526,
          0.6857255016293125,
          0.667430715394179,
          0.6468009397841309,
          0.6921794504624222,
          0.7174691332389604,
          0.706589484387581,
          0.7418768852097439
         ]
        },
        {
         "line": {
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "markers+lines",
         "name": "GPT-2",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23
         ],
         "y": [
          0.9508724593831297,
          0.9264606144587894,
          0.899361592500509,
          0.8403586155920163,
          0.7562098548860117,
          0.6949593460081518,
          0.6628160918536098,
          0.639305294722998,
          0.6246413963503724,
          0.6035428556090265,
          0.5845363248099831,
          0.5672975548153812,
          0.5702460898938737,
          0.5760687966171792,
          0.5777400429593116,
          0.5737599671079956,
          0.562389093012651,
          0.5556394648321298,
          0.5611628636802414,
          0.5675801260266029,
          0.587584449882135,
          0.6121093601281207,
          0.6497593039039014,
          0.6594467477012638
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Model"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Context-Specificity Comparison: BERT vs GPT-2"
        },
        "xaxis": {
         "title": {
          "text": "Layer Number"
         }
        },
        "yaxis": {
         "title": {
          "text": "Context-Specificity Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    'Layer': list(range(num_layers_bert)) + list(range(num_layers_gpt)),\n",
    "    'Context': bert_context + gpt_context,\n",
    "    'Model': ['BERT'] * num_layers_bert + ['GPT-2'] * num_layers_gpt\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'BERT']['Layer'], \n",
    "                         y=df[df['Model'] == 'BERT']['Context'],\n",
    "                         mode='markers+lines',\n",
    "                         name='BERT',\n",
    "                         line=dict(shape='linear', dash='dot')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[df['Model'] == 'GPT-2']['Layer'], \n",
    "                         y=df[df['Model'] == 'GPT-2']['Context'],\n",
    "                         mode='markers+lines',\n",
    "                         name='GPT-2',\n",
    "                         line=dict(shape='linear', dash='dot')))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Context-Specificity Comparison: BERT vs GPT-2\",\n",
    "    xaxis_title=\"Layer Index\",\n",
    "    yaxis_title=\"Context-Specificity Value\",\n",
    "    legend_title=\"Model\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm, trange\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function - funkcje pomocnicze z notebook'a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALNUM_CHARSET = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "\n",
    "def convert_to_tokens(indices, tokenizer, extended=False, extra_values_pos=None, strip=True):\n",
    "    if extended:\n",
    "        res = [tokenizer.convert_ids_to_tokens([idx])[0] if idx < len(tokenizer) else \n",
    "               (f\"[pos{idx-len(tokenizer)}]\" if idx < extra_values_pos else f\"[val{idx-extra_values_pos}]\") \n",
    "               for idx in indices]\n",
    "    else:\n",
    "        res = tokenizer.convert_ids_to_tokens(indices)\n",
    "    if strip:\n",
    "        res = list(map(lambda x: x[1:] if x[0] == '' else \"#\" + x, res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_tokens(v, k=100, tokenizer=None, only_alnum=False, only_ascii=True, with_values=False, \n",
    "               exclude_brackets=False, extended=True, extra_values=None, only_from_list=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    v = deepcopy(v)\n",
    "    ignored_indices = []\n",
    "    if only_ascii:\n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not val.strip('').isascii()])\n",
    "    if only_alnum: \n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not (set(val.strip('[] ')) <= ALNUM_CHARSET)])\n",
    "    if only_from_list:\n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if val.strip(' ').lower() not in only_from_list])\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "        \n",
    "    ignored_indices = list(set(ignored_indices))\n",
    "    v[ignored_indices] = -np.inf\n",
    "    extra_values_pos = len(v)\n",
    "    if extra_values is not None:\n",
    "        v = torch.cat([v, extra_values])\n",
    "    values, indices = torch.topk(v, k=k)\n",
    "    res = convert_to_tokens(indices, tokenizer, extended=extended, extra_values_pos=extra_values_pos)\n",
    "    if with_values:\n",
    "        res = list(zip(res, values.cpu().numpy()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Weights GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"sdadas/polish-gpt2-medium\",\n",
       "  \"activation_function\": \"gelu_fast\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 1024,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": 4096,\n",
       "  \"n_layer\": 24,\n",
       "  \"n_positions\": 2048,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"tokenizer_class\": \"GPT2TokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 51200\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"sdadas/polish-gpt2-medium\")\n",
    "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(\"sdadas/polish-gpt2-medium\")\n",
    "emb = model.get_output_embeddings().weight.data.T.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 51200])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfugracja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"sdadas/polish-gpt2-medium\",\n",
       "  \"activation_function\": \"gelu_fast\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 1024,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": 4096,\n",
       "  \"n_layer\": 24,\n",
       "  \"n_positions\": 2048,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"tokenizer_class\": \"GPT2TokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 51200\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architektura modelu GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 1024)\n",
       "    (wpe): Embedding(2048, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): FastGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ekstrackja wag z modelu GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_inv torch.Size([51200, 1024])\n"
     ]
    }
   ],
   "source": [
    "num_layers = model.config.n_layer\n",
    "num_heads = model.config.n_head\n",
    "hidden_dim = model.config.n_embd\n",
    "head_size = hidden_dim // num_heads\n",
    "\n",
    "K = torch.cat(\n",
    "    [\n",
    "        model.get_parameter(f\"transformer.h.{j}.mlp.c_fc.weight\").T\n",
    "        for j in range(num_layers)\n",
    "    ]\n",
    ").detach()\n",
    "V = torch.cat(\n",
    "    [\n",
    "        model.get_parameter(f\"transformer.h.{j}.mlp.c_proj.weight\")\n",
    "        for j in range(num_layers)\n",
    "    ]\n",
    ").detach()\n",
    "\n",
    "W_Q, W_K, W_V = (\n",
    "    torch.cat(\n",
    "        [\n",
    "            model.get_parameter(f\"transformer.h.{j}.attn.c_attn.weight\")\n",
    "            for j in range(num_layers)\n",
    "        ]\n",
    "    )\n",
    "    .detach()\n",
    "    .chunk(3, dim=-1)\n",
    ")\n",
    "W_O = torch.cat(\n",
    "    [\n",
    "        model.get_parameter(f\"transformer.h.{j}.attn.c_proj.weight\")\n",
    "        for j in range(num_layers)\n",
    "    ]\n",
    ").detach()\n",
    "\n",
    "K_heads = K.reshape(num_layers, -1, hidden_dim)\n",
    "V_heads = V.reshape(num_layers, -1, hidden_dim)\n",
    "d_int = K_heads.shape[1]\n",
    "\n",
    "W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(\n",
    "    0, 2, 1, 3\n",
    ")\n",
    "W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(\n",
    "    0, 2, 1, 3\n",
    ")\n",
    "W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(\n",
    "    0, 2, 1, 3\n",
    ")\n",
    "W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)\n",
    "\n",
    "emb_inv = emb.T\n",
    "print(\"emb_inv\", emb_inv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretacja wag modelu GPT2 na pustej licie tokenw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FF Keys and Values - Klucze i wartoci funkcji Feed Forward\n",
    "- Klucze funkcji feed-forward (FF keys): rzutowane s przez pomnoenie przez transpozycj prawostronnej odwrotnoci macierzy osadze (KET)\n",
    "- Wartoci funkcji feed-forward (FF values): rzutowane s przez pomnoenie przez macierz osadze E.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers in the model: 24\n",
      "Neurons in the model: 4096\n"
     ]
    }
   ],
   "source": [
    "print(f\"Layers in the model: {num_layers}\")\n",
    "print(f\"Neurons in the model: {model.config.n_inner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4096, 1024])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4096, 1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_top_tokens(i1, i2, K_heads, V_heads, emb, tokens_list=None):\n",
    "\n",
    "    print(f'Layer {i1}, Neuron {i2}')\n",
    "    print(\n",
    "        tabulate(\n",
    "            [\n",
    "                *zip(\n",
    "                    top_tokens(\n",
    "                        (K_heads[i1, i2]) @ emb,\n",
    "                        k=30,\n",
    "                        only_from_list=tokens_list,\n",
    "                        only_alnum=False,\n",
    "                    ),\n",
    "                    top_tokens(\n",
    "                        (V_heads[i1, i2]) @ emb,\n",
    "                        k=30,\n",
    "                        only_from_list=tokens_list,\n",
    "                        only_alnum=False,\n",
    "                    ),\n",
    "                    top_tokens((-K_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list),\n",
    "                    top_tokens((-V_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list),\n",
    "                )\n",
    "            ],\n",
    "            headers=[\"K\", \"V\", \"-K\", \"-V\"],\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer index and neuron index - indeks warstwy i indeks neuronu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 23, Neuron 907\n",
      "K          V              -K          -V\n",
      "---------  -------------  ----------  ---------\n",
      "przycu     kody           dotychczas  #bot\n",
      "zalog      #gory          rodzi       #lot\n",
      "wylegi     #ei            do          #ush\n",
      "#walifik   #zmy           przebie     #dzista\n",
      "przesp     Apo            gatunku     #remont\n",
      "pochowany  apokali        przez       #lee\n",
      "#cket      #128           #ja         #wan\n",
      "#ppe       ludy           rodzin      #ette\n",
      "wep        #ords          zrazu       #up\n",
      "sfinans    Cezary         lokalnie    #bul\n",
      "#iss       archa          porywa      #puszczam\n",
      "#CS        przy           two         spu\n",
      "#gny       akcy           jeszcze     zamyka\n",
      "#zwol      Homo           #jaw        odstawi\n",
      "#-).       litera         na          #mont\n",
      "lock       #pka           wymaga      #beki\n",
      "skonfisk   Cezar          ty          #laks\n",
      "#post      Benedykt       Drze        tap\n",
      "postoju    narodem        pod         poby\n",
      "erek       polityki       Nie         posto\n",
      "#ionu      symbolu        rzuca       #wana\n",
      "#ppo       #lachet        ludzkim     #dzana\n",
      "unierucho  publicznego    uczucia     #load\n",
      "#ott       #rzami         rze         #owol\n",
      "zbombar    anie           tak         zatk\n",
      "#FF        uniwersalny    da          opuszcza\n",
      "#prem      artystycznego  ludzkie     znajd\n",
      "#klaski    ustawowego     natury      #dzany\n",
      "#gs        werb           #obie       #elli\n",
      "zatk       wzroku         pewnego     #bek\n"
     ]
    }
   ],
   "source": [
    "# Przykad z notebook'a\n",
    "ilayer = 23\n",
    "ineuron = 907\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 23, Neuron 4095\n",
      "K         V            -K    -V\n",
      "--------  -----------  ----  ----\n",
      "#biego    #Olbrzym     d     K\n",
      "#bieni    #Dzwonek     przy  #y\n",
      "#arki     #Obywat      z     S\n",
      "#Byli     #Naraz       i     Hu\n",
      "#bione    #Organiz     nie   Ko\n",
      "#biny     #Uczestni    po    Z\n",
      "#Stacja   #Ostatnim    w     Ch\n",
      "#basy     #Nikomu      to    Gu\n",
      "#atki     #Zgoda       od    B\n",
      "#bice     #Robert      hu    GU\n",
      "#wory     #Aktor       #,    Ta\n",
      "#HC       #Weszli      de    Lan\n",
      "#CJE      #Plat        ku    Mer\n",
      "#dziny    #Same        do    D\n",
      "#anami    #Idziemy     sz    G\n",
      "#hony     #Jakbym      u     o\n",
      "#bowych   #polsce      l     Sm\n",
      "#biony    #Jakby       ze    M\n",
      "#BIE      #Lokal       Hu    #e\n",
      "#body     #aby         e     U\n",
      "#bane     #Zdro        mat   Mac\n",
      "#noscia   #chodzimy    kur   Han\n",
      "#Pisze    #Coraz       a     #xa\n",
      "#lektory  #Policj      na    SM\n",
      "#beli     #Inwestycja  I     AB\n",
      "#Gmina    #Faj         O     T\n",
      "#ARE      #Zbiera      pod   O\n",
      "#Drzewa   #Noc         tego  #ki\n",
      "#GBT      #Nat         g     Bry\n",
      "#bioty    #Idzie       Ku    Ha\n"
     ]
    }
   ],
   "source": [
    "# ostatni neuron w ostatniej warstwie\n",
    "ilayer = 23\n",
    "ineuron = 4095\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "K        V            -K             -V\n",
      "-------  -----------  -------------  ---------\n",
      "#uel     #pus         Tod            #plek\n",
      "#ul      #ont         #skom          #strzy\n",
      "#uela    #onek        podzie         #kli\n",
      "#pul     #sud         #dzie          #wat\n",
      "#gul     #onych       #sce           #liwie\n",
      "#ann     #TK          #wie           #rgi\n",
      "bel      #STWO        #max           uszy\n",
      "#jal     #oper        #zenia         #RI\n",
      "#cht     #oty         #stycznych     #las\n",
      "#oll     #BW          rezerwat       #gre\n",
      "#El      #ARS         predy          podze\n",
      "#ulf     #poty        #system        #stralij\n",
      "El       #ones        niespodzianki  #dle\n",
      "el       postoju      niespodzianka  #nisz\n",
      "dywiden  #det         #styczne       #dney\n",
      "#alu     #sbur        #kaza          #genera\n",
      "#atak    konspiracji  #rom           #inga\n",
      "#itt     #ono         ciekawostki    rozdz\n",
      "#chol    #pes         #rick          usposobie\n",
      "#anga    #rzac        #dac           #rwi\n",
      "#al      ognisk       przygod        #rodzi\n",
      "#eman    #akami       #zna           #wska\n",
      "#olony   #ematy       #szki          #isa\n",
      "#aret    #eryk        fotografi      naba\n",
      "#EL      #orus        #zka           #szny\n",
      "#CJ      #szczyzna    #jazdu         #rk\n",
      "#olu     #pet         #zgi           #nota\n",
      "Cul      Algier       listy          #nastu\n",
      "#ulu     #ologia      #szcze         #wum\n",
      "#amp     #lanki       #dziemy        #szkach\n"
     ]
    }
   ],
   "source": [
    "i1, i2 = 0, 0\n",
    "print_top_tokens(i1, i2, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covid i kraje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2, Neuron 1654\n",
      "K            V           -K        -V\n",
      "-----------  ----------  --------  ---------\n",
      "#soci        Tob         #czynie   stopnia\n",
      "immuno       habili      #liny     #pty\n",
      "Demokracji   #mund       #iali     #day\n",
      "newslet      Bag         #ujecie   sani\n",
      "#walony      Sztokhol    #Ul       konserw\n",
      "byle         Kopenha     #linie    #ore\n",
      "emerytalne   #isy        #czyny    #anci\n",
      "COVID        operacyjny  #olan     #ptom\n",
      "rektora      Leningra    Bell      #antka\n",
      "podro        norwe       #line     parki\n",
      "glowa        #lda        #57       spos\n",
      "covid        Sopo        #id       #peratura\n",
      "#rek         #targ       #bar      #leja\n",
      "Trybun       #sio        #tonu     #lee\n",
      "#ytany       #szen       #lic      #post\n",
      "Europejskim  Wolf        #ian      #walent\n",
      "Uniwersyte   kame        #osenki   #pcie\n",
      "panstwa      szwe        #52       #stadt\n",
      "#Nikogo      autorskie   #gramu    #oro\n",
      "ryja         Bere        #belki    #lecz\n",
      "#misja       ptak        #ai       #pania\n",
      "Niego        #szeniach   #pala     #lizuje\n",
      "Zygmu        Oslo        #56       #ody\n",
      "wodza        Wrze        #odi      #loria\n",
      "konca        Moz         #ofer     #pano\n",
      "emerytalnym  #Mir        #netki    #stry\n",
      "pleba        #kowca      #problem  #ptem\n",
      "#bro         miod        #gol      #pita\n",
      "#rusem       dysk        odbior    #enty\n",
      "akademickim  umorzeniu   #ea       #stad\n"
     ]
    }
   ],
   "source": [
    "ilayer = 2\n",
    "ineuron = 1654\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przestpstwa? kryminalistyka?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 22, Neuron 3360\n",
      "K            V       -K         -V\n",
      "-----------  ------  ---------  -----------\n",
      "narkoty      #tos    rozmie     #redy\n",
      "#zwykopem    #TM     #aku       #oner\n",
      "#boli        #czek   G          #tyw\n",
      "alkoholizm   #128    #d         #illi\n",
      "#eksyka      #dnik   Fryd       #nera\n",
      "halu         #Http   Kl         #Kultura\n",
      "przedaw      #rowy   #th        #syw\n",
      "porwania     #razem  #harmo     emigracji\n",
      "#ect         szczeg  M          kampanii\n",
      "narko        #miast  wojewody   autorskich\n",
      "pej          Byto    rozlo      demokra\n",
      "narkotyki    #gua    dobud      #pij\n",
      "#nezyj       Get     Dwo        #iry\n",
      "#bolu        ogniem  poddanych  urlopu\n",
      "#Poszuki     #czke   Lit        #UN\n",
      "halucyna     #ciu    Th         #Kandy\n",
      "#tnamie      #raz    #ospo      #zy\n",
      "narkotykami  Stal    K          #bur\n",
      "afera        #razu   Gale       referen\n",
      "#tnam        rzek    #St        #tywa\n",
      "dopala       #dalem  #polska    #iller\n",
      "seksualne    Sad     Pa         #ersona\n",
      "gangster     #dniki  wiernie    celu\n",
      "temat        wydech  Fryderyka  Zjednoczone\n",
      "kredytowych  #cs     #god       #ori\n",
      "#bole        #vil    Clar       #etry\n",
      "depresja     #lce    #nowo      autor\n",
      "#Zapad       #rze    uszan      #tywy\n",
      "intensywne   paliw   H          urlopie\n",
      "marihu       #czasz  C          turysty\n"
     ]
    }
   ],
   "source": [
    "ilayer = 22\n",
    "ineuron = 3360\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teorie spiskowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12, Neuron 3763\n",
      "K           V           -K          -V\n",
      "----------  ----------  ----------  -------------\n",
      "porwania    Smole       wachlarz    #ymon\n",
      "FBI         #rzany      staw        #pen\n",
      "samolotu    smole       #nop        #amu\n",
      "#rory       #rory       #styki      #kiel\n",
      "#ash        UFO         Cuk         #eno\n",
      "NSA         #rany       #mus        akompa\n",
      "zestrze     Marsa       #klasy      niskie\n",
      "podkomisji  wahad       wstaw       niskich\n",
      "lotu        Czecze      #styka      administracji\n",
      "afery       #had        oliw        drewno\n",
      "#hera       wirusa      stawu       drew\n",
      "#Charlie    #ralni      wydziela    odczu\n",
      "#ond        abor        #lator      #chol\n",
      "terrorysty  #rzani      #stw        #eny\n",
      "inwi        sekty       korzeni     #ymu\n",
      "#154        porwania    wzniesi     #fonii\n",
      "#lotem      WSI         wargach     #standar\n",
      "rejs        Katy        #mienia     monoton\n",
      "#ulf        #TA         #szczyzna   odzwierciedla\n",
      "CIA         #dliwie     #kolor      desek\n",
      "samolocie   #biec       ()          #tero\n",
      "uprowa      #rika       wsta        deski\n",
      "szpie       #rice       cuk         pieszczot\n",
      "samolot     ISIS        #lacyjne    #neko\n",
      "mordercy    #urat       #stawienie  ujem\n",
      "morderstwa  zamachu     utrzym      edy\n",
      "#nger       #vet        staro       przyzwyczaj\n",
      "#had        #less       Powiat      deskach\n",
      "#Zamach     lotniczej   #ksz        dziennym\n",
      "inwigi      furgonetki  ustach      pochleb\n"
     ]
    }
   ],
   "source": [
    "ilayer = 12\n",
    "ineuron = 3763\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afganistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 5, Neuron 2031\n",
      "K               V          -K           -V\n",
      "--------------  ---------  -----------  -----------\n",
      "Afganistanu     Rzesz      #reb         emerytalne\n",
      "Afga            #parcie    fair         publiczne\n",
      "Jehowy          #tyz       #gle         #szto\n",
      "Wojny           #stie      #cie         ui\n",
      "Obywatelskich   #tz        #bal         #smo\n",
      "zbiorowych      #kop       #patrz       Sub\n",
      "etni            #tkami     #arter       #szard\n",
      "getta           #zdro      #gl          publiczna\n",
      "rdzenia         #lock      moimi        odprawy\n",
      "#niegdzie       #tto       mym          powsze\n",
      "Kinga           #TS        #bki         #Ur\n",
      "etnicznych      #wcze      moim         autorskie\n",
      "Afganistanie    #ieu       #pi          Anne\n",
      "wojnach         uderzeniu  #de          #mion\n",
      "#zacje          rzesz      #lar         integra\n",
      "przegranej      #sci       pas          ur\n",
      "emerytalnych    #zej       biurko       publicznych\n",
      "wojennych       #zno       #icho        celne\n",
      "Krajowych       hamowania  #dgo         Ern\n",
      "zbiorowe        #stres     #dalej       #wymiar\n",
      "uznanych        #oro       #czu         zawodowo\n",
      "mieszkaniowych  Lucas      #step        bary\n",
      "Sary            szta       #skaki       publiczny\n",
      "Polsko          #sterze    #trzym       #owicie\n",
      "nieulecz        kalenda    #dzwo        #Jon\n",
      "obywat          #tek       znajdziecie  var\n",
      "podatkowa       #chor      #rodziej     fel\n",
      "granicznych     diecezji   #bek         uboczne\n",
      "Albanii         #tora      spostrze     #rekty\n",
      "wojny           oparciu    #ather       #anial\n"
     ]
    }
   ],
   "source": [
    "ilayer = 5\n",
    "ineuron = 2031\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nazwy imiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 19, Neuron 0\n",
      "K            V              -K      -V\n",
      "-----------  -------------  ------  --------\n",
      "imiona       pseudonim      #a      #uszka\n",
      "imion        nazwy          so      #tari\n",
      "nazw         #naz           #to     #usze\n",
      "nazwa        nazw           na      #strzy\n",
      "pseudonim    krypto         w       Kore\n",
      "nazwy        przydomek      #,      #lander\n",
      "przydomek    imienia        #lo     Cmenta\n",
      "nazwisko     nazwa          #li     #rzymie\n",
      "#Nazwa       nazew          i       #taria\n",
      "imienia      imion          rewi    #bara\n",
      "nazwisk      imiona         al      czem\n",
      "pseudonimem  oznaczenie     #c      Serce\n",
      "oznaczenie   nazwisko       b       #aga\n",
      "imieniem     pseudonimem    #lota   Gier\n",
      "termino      kod            #letni  #wi\n",
      "nazwiskiem   specjalizacji  do      #uszek\n",
      "nazew        #Nazwa         #kor    #ariuszy\n",
      "nazwiska     przydom        ra      jo\n",
      "nazywam      #okre          bram    ustrze\n",
      "nazwi        termino        #k      #ymber\n",
      "nazwali      nume           wy      Atenach\n",
      "nazwano      numery         Pa      #ranie\n",
      "#Nazywam     numeru         #f      Aten\n",
      "#naz         kodu           #.      #ecka\n",
      "nazwisku     nadano         #lin    #ett\n",
      "nazywali     imie           ar      zawali\n",
      "#Naz         #px            r       #anka\n",
      "nazywano     nazwano        #gali   #wiercie\n",
      "nazwie       nazwiska       #ren    Radzi\n",
      "tytule       nazwie         wal     #kowie\n"
     ]
    }
   ],
   "source": [
    "ilayer = 19\n",
    "ineuron = 0\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ameryka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 19, Neuron 1035\n",
      "K             V         -K        -V\n",
      "------------  --------  --------  ----------\n",
      "#Trump        Aut       #sz       #yj\n",
      "#Bill         #daci     #beli     #towa\n",
      "Jackson       #niach    Cza       #ansu\n",
      "American      die       #wieni    #ull\n",
      "Waszyngtonie  #VD       #wicz     #turem\n",
      "Josh          bio       Strze     #ty\n",
      "nowojor       #dym      Ko        Szczecina\n",
      "#Ameryka      #die      #dowska   #tyna\n",
      "Kennedy       #dia      #rgi      #sik\n",
      "Carter        #niczymi  mundu     Szczecinie\n",
      "#Produkty     #dzeni    strze     #jonu\n",
      "#Will         #zdni     Bere      #szek\n",
      "Warren        #ica      #wi       #rum\n",
      "Trump         Abdu      #nt       #cjum\n",
      "#neya         ulot      patro     Szczepa\n",
      "Kalifor       #niczym   najpierw  #siki\n",
      "#John         Hom       Gie       #ulu\n",
      "Vegas         lek       gi        #szczek\n",
      "#hire         #dzie     Bu        #eze\n",
      "Wash          #ninie    Woj       #ucha\n",
      "Angeles       #de       dach      #szt\n",
      "#okracji      #twem     #r        #Lata\n",
      "Davis         Gol       posze     #ponu\n",
      "#ansas        #lica     Kamie     #anowa\n",
      "Waszyngton    mieliby   #czyli    #tyn\n",
      "Trumpa        #nieni    #chro     #hl\n",
      "Kalifornii    stan      wsta      #rowa\n",
      "#Goo          #nicy     #lt       #atarzy\n",
      "#William      #dine     posa      #row\n",
      "Waszyng       #dni      Ojczy     #si\n"
     ]
    }
   ],
   "source": [
    "ilayer = 19\n",
    "ineuron = 1035\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miejsca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12, Neuron 970\n",
      "K             V           -K          -V\n",
      "------------  ----------  ----------  -------------\n",
      "Kolumbii      Spring      #lnik       #zes\n",
      "polonij       Chal        przeciwnik  #bour\n",
      "Estonii       Kosza       #bek        #owatych\n",
      "Alba          Spa         #czaj       #szcie\n",
      "aero          Pompej      #dziec      #rzego\n",
      "alpej         Nort        #tch        polityczny\n",
      "Lubli         Algier      #lna        #atyw\n",
      "spalin        Chrz        #lt         dyskryminacji\n",
      "Katowicach    Cez         #za         wystawy\n",
      "#hausen       Gdynia      #l          rekomendacji\n",
      "epizo         #nton       #lowi       turystyczny\n",
      "berli         #nit        #gon        #anki\n",
      "Esto          Sur         #chta       poselski\n",
      "gospodarstwa  #nian       #Jego       opublikowany\n",
      "Katowice      #nity       #zyc        artystyczny\n",
      "Sano          Ari         #kusa       ustawowych\n",
      "NRD           #nowiec     #zy         odrzucenia\n",
      "#derni        Stanach     #ego        #towanych\n",
      "benzy         #uston      #resz       podatnika\n",
      "Austr         Atenach     #lego       artystycznego\n",
      "Moder         Bal         #hak        #jazdy\n",
      "Argenty       Kielce      #liki       wydania\n",
      "RFN           Cro         #resji      #komen\n",
      "tytoni        Arka        #ens        dyrekcji\n",
      "Donbas        Mars        #zw         #iery\n",
      "Regional      #woka       #czna       podatnik\n",
      "gospodarstwo  1926        #chwy       kampanii\n",
      "benzyn        Olsztyn     strzec      #==\n",
      "kukurydzy     Washington  #tka        przyjmowania\n",
      "gospodarstw   Poznaniu    #enia       emerytalny\n"
     ]
    }
   ],
   "source": [
    "ilayer = 12\n",
    "ineuron = 970\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12, Neuron 970\n",
      "K             V           -K          -V\n",
      "------------  ----------  ----------  -------------\n",
      "Kolumbii      Spring      #lnik       #zes\n",
      "polonij       Chal        przeciwnik  #bour\n",
      "Estonii       Kosza       #bek        #owatych\n",
      "Alba          Spa         #czaj       #szcie\n",
      "aero          Pompej      #dziec      #rzego\n",
      "alpej         Nort        #tch        polityczny\n",
      "Lubli         Algier      #lna        #atyw\n",
      "spalin        Chrz        #lt         dyskryminacji\n",
      "Katowicach    Cez         #za         wystawy\n",
      "#hausen       Gdynia      #l          rekomendacji\n",
      "epizo         #nton       #lowi       turystyczny\n",
      "berli         #nit        #gon        #anki\n",
      "Esto          Sur         #chta       poselski\n",
      "gospodarstwa  #nian       #Jego       opublikowany\n",
      "Katowice      #nity       #zyc        artystyczny\n",
      "Sano          Ari         #kusa       ustawowych\n",
      "NRD           #nowiec     #zy         odrzucenia\n",
      "#derni        Stanach     #ego        #towanych\n",
      "benzy         #uston      #resz       podatnika\n",
      "Austr         Atenach     #lego       artystycznego\n",
      "Moder         Bal         #hak        #jazdy\n",
      "Argenty       Kielce      #liki       wydania\n",
      "RFN           Cro         #resji      #komen\n",
      "tytoni        Arka        #ens        dyrekcji\n",
      "Donbas        Mars        #zw         #iery\n",
      "Regional      #woka       #czna       podatnik\n",
      "gospodarstwo  1926        #chwy       kampanii\n",
      "benzyn        Olsztyn     strzec      #==\n",
      "kukurydzy     Washington  #tka        przyjmowania\n",
      "gospodarstw   Poznaniu    #enia       emerytalny\n"
     ]
    }
   ],
   "source": [
    "ilayer, ineuron = np.random.randint(num_layers), np.random.randint(d_int)\n",
    "\n",
    "print_top_tokens(ilayer, ineuron, K_heads, V_heads, emb, tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Weights interpretation - Interpretacja wag modulu atencji\n",
    "\n",
    "Rzutujc parametry do przestrzeni osadze, mona je interpretowa w kategoriach interakcji midzy sowami ze sownika. Na przykad:\n",
    "- W przypadku macierzy WQK mona zidentyfikowa pary sw, ktre silnie ze sob wspgraj w kontekcie mechanizmu uwagi.\n",
    "- W przypadku macierzy WVO mona zobaczy, jak uwaga na pewne sowa wpywa na kolejne ukryte stany modelu, a co za tym idzie, na ostateczne przewidywania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disclaimer:\n",
    "\n",
    "W praktyce autorzy uywaj transpozycji macierzy osadze (ET) jako prawostronnej odwrotnoci E, zamiast bardziej odpowiedniej teoretycznie pseudo-odwrotnoci (E+). Dzieje si tak, poniewa interpretacja w przestrzeni osadze czsto polega na wybieraniu k najwaniejszych elementw (top-k), a pseudo-odwrotno nie zachowuje si dobrze w poczeniu z t operacj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_topk(mat, min_k=500, max_k=250_000, th0=10, max_iters=10, verbose=False):\n",
    "    _get_actual_k = lambda th, th_max: torch.nonzero((mat > th) & (mat < th_max)).shape[\n",
    "        0\n",
    "    ]\n",
    "    th_max = np.inf\n",
    "    left, right = 0, th0\n",
    "    while True:\n",
    "        actual_k = _get_actual_k(right, th_max)\n",
    "        if verbose:\n",
    "            print(f\"one more iteration. {actual_k}\")\n",
    "        if actual_k <= max_k:\n",
    "            break\n",
    "        left, right = right, right * 2\n",
    "    if min_k <= actual_k <= max_k:\n",
    "        th = right\n",
    "    else:\n",
    "        for _ in range(max_iters):\n",
    "            mid = (left + right) / 2\n",
    "            actual_k = _get_actual_k(mid, th_max)\n",
    "            if verbose:\n",
    "                print(f\"one more iteration. {actual_k}\")\n",
    "            if min_k <= actual_k <= max_k:\n",
    "                break\n",
    "            if actual_k > max_k:\n",
    "                left = mid\n",
    "            else:\n",
    "                right = mid\n",
    "        th = mid\n",
    "    return torch.nonzero((mat > th) & (mat < th_max)).tolist()\n",
    "\n",
    "\n",
    "def get_top_entries(\n",
    "    tmp,\n",
    "    all_high_pos,\n",
    "    only_ascii=False,\n",
    "    only_alnum=False,\n",
    "    exclude_same=False,\n",
    "    exclude_fuzzy=False,\n",
    "    tokens_list=None,\n",
    "    reverse_list=False,\n",
    "):\n",
    "    remaining_pos = all_high_pos\n",
    "    if only_ascii:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: (\n",
    "                    tokenizer.decode(x[0]).strip(\"\").isascii()\n",
    "                    and tokenizer.decode(x[1]).strip(\"\").isascii()\n",
    "                ),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "    if only_alnum:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: (\n",
    "                    tokenizer.decode(x[0]).strip(\" \").isalnum()\n",
    "                    and tokenizer.decode(x[1]).strip(\" \").isalnum()\n",
    "                ),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "    if exclude_same:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: tokenizer.decode(x[0]).lower().strip()\n",
    "                != tokenizer.decode(x[1]).lower().strip(),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "    if exclude_fuzzy:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: not _fuzzy_eq(\n",
    "                    tokenizer.decode(x[0]).lower().strip(),\n",
    "                    tokenizer.decode(x[1]).lower().strip(),\n",
    "                ),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "    if tokens_list:\n",
    "        remaining_pos = [\n",
    "            *filter(\n",
    "                lambda x: (\n",
    "                    (tokenizer.decode(x[0]).strip(\"\").lower().strip() in tokens_list)\n",
    "                    and (\n",
    "                        tokenizer.decode(x[1]).strip(\"\").lower().strip()\n",
    "                        in tokens_list\n",
    "                    )\n",
    "                ),\n",
    "                remaining_pos,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    pos_val = tmp[[*zip(*remaining_pos)]]\n",
    "    good_cells = [\n",
    "        *map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos)\n",
    "    ]\n",
    "    good_tokens = list(map(lambda x: Counter(x).most_common(), zip(*good_cells)))\n",
    "    remaining_pos_best = np.array(remaining_pos)[\n",
    "        torch.argsort(pos_val if reverse_list else -pos_val)[:50]\n",
    "    ]\n",
    "    good_cells_best = [\n",
    "        *map(\n",
    "            lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])),\n",
    "            remaining_pos_best,\n",
    "        )\n",
    "    ]\n",
    "    # good_cells[:100]\n",
    "    # list(zip(good_tokens[0], good_tokens[1]))\n",
    "    return good_cells_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WVO Interpretation\n",
    "Interpretacja Wvo polega na analizie tej macierzy przejcia, identyfikujc pary sw, ktre s silnie ze sob powizane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_weights_vo(i1, i2, exclude_same=False, reverse_list=False, only_ascii=True, only_alnum=False, tokens_list=None):\n",
    "    print(f\"Layer {i1}, Head {i2}\")\n",
    "\n",
    "    # Extract the weights for the specified layer and neuron\n",
    "    W_V_tmp, W_O_tmp = W_V_heads[i1, i2, :], W_O_heads[i1, i2]\n",
    "    \n",
    "    # Compute the temporary matrix using emb_inv\n",
    "    tmp = emb_inv @ (W_V_tmp @ W_O_tmp) @ emb\n",
    "    \n",
    "    # Approximate top-k values\n",
    "    all_high_pos = approx_topk(tmp, th0=1, verbose=True)\n",
    "    \n",
    "    # Get top entries based on the specified parameters\n",
    "    top_entries = get_top_entries(\n",
    "        tmp,\n",
    "        all_high_pos,\n",
    "        only_ascii=only_ascii,\n",
    "        only_alnum=only_alnum,\n",
    "        exclude_same=exclude_same,\n",
    "        tokens_list=tokens_list,\n",
    "        reverse_list=reverse_list\n",
    "    )\n",
    "    \n",
    "    return top_entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przedrostki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 10\n",
      "one more iteration. 5182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' ranem', ' nad'),\n",
       " ('ornie', ' przez'),\n",
       " ('datek', ' nad'),\n",
       " ('arl', ' przed'),\n",
       " (' pewno', ' na'),\n",
       " ('spodziewanie', ' nad'),\n",
       " (' razu', ' od'),\n",
       " ('miernie', ' nad'),\n",
       " (' okazji', ' przy'),\n",
       " (' wsk', ' na'),\n",
       " (' czele', ' na'),\n",
       " ('orne', ' przez'),\n",
       " ('godziny', ' nad'),\n",
       " ('sione', ' przed'),\n",
       " (' ranem', 'nad'),\n",
       " ('natural', ' nad'),\n",
       " ('przewo', ' nad'),\n",
       " (' Duna', ' nad'),\n",
       " ('ktory', ' do'),\n",
       " ('miar', ' nad'),\n",
       " (' dobra', ' dla'),\n",
       " (' Jezi', ' nad'),\n",
       " ('spodzie', ' nad'),\n",
       " (' niedawna', ' do'),\n",
       " ('pisie', ' pod'),\n",
       " (' wygody', ' dla'),\n",
       " ('arcie', ' przed'),\n",
       " (' sumie', ' w'),\n",
       " ('miernie', 'nad'),\n",
       " ('tek', ' pod'),\n",
       " ('wcze', ' przed'),\n",
       " ('mier', ' nad'),\n",
       " ('hala', ' pod'),\n",
       " ('ornie', ' przeze'),\n",
       " (' uboczu', ' na'),\n",
       " (' podstawie', ' na'),\n",
       " (' ranem', 'Nad'),\n",
       " ('czesne', ' do'),\n",
       " ('granicznych', ' nad'),\n",
       " ('wiska', ' przez'),\n",
       " (' barkach', ' na'),\n",
       " ('ornie', 'przez'),\n",
       " (' odmiany', ' dla'),\n",
       " ('niego', ' przed'),\n",
       " (' plecami', ' za'),\n",
       " (' dobi', ' na'),\n",
       " ('przewodni', ' nad'),\n",
       " (' koniec', ' pod'),\n",
       " (' razie', ' na'),\n",
       " (' potrzeby', ' na')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "ilayer, head = 21, 7\n",
    "analyze_weights_vo(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=False,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "swoich swoje siebie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 11, Head 12\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 13\n",
      "one more iteration. 15010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' komukolwiek', ' komukolwiek'),\n",
       " ('siebie', 'siebie'),\n",
       " (' swoich', ' swoich'),\n",
       " (' swojej', ' swojej'),\n",
       " (' swej', ' swojej'),\n",
       " ('scie', 'scie'),\n",
       " (' swojej', ' swoich'),\n",
       " ('ingu', 'ingu'),\n",
       " (' swoje', ' swoich'),\n",
       " (' swoich', ' swojej'),\n",
       " (' swoja', ' swoich'),\n",
       " (' swych', ' swoich'),\n",
       " (' swoja', ' swojej'),\n",
       " (' siebie', ' siebie'),\n",
       " (' swoja', ' swoim'),\n",
       " (' swej', ' swej'),\n",
       " (' swoim', ' swoim'),\n",
       " (' swych', ' swojej'),\n",
       " (' bezkarnie', ' bezkarnie'),\n",
       " ('siebie', ' siebie'),\n",
       " ('rko', 'rko'),\n",
       " (' swych', ' swych'),\n",
       " (' swoje', ' swoje'),\n",
       " ('imon', 'imon'),\n",
       " (' swoich', ' swych'),\n",
       " (' siebie', 'siebie'),\n",
       " (' swej', ' swoich'),\n",
       " ('bes', 'bes'),\n",
       " (' sobie', ' sobie'),\n",
       " (' swej', ' swych'),\n",
       " (' swoje', ' swojej'),\n",
       " ('dun', 'dun'),\n",
       " (' swojej', ' swych'),\n",
       " (' swoim', ' swojej'),\n",
       " (' swojej', ' swoim'),\n",
       " (' swojej', ' swej'),\n",
       " (' swoje', ' swoim'),\n",
       " (' swoim', ' swoich'),\n",
       " (' swojego', ' swoich'),\n",
       " (' swoja', ' swych'),\n",
       " ('ujecie', 'ujecie'),\n",
       " (' beze', ' beze'),\n",
       " (' swoich', ' swojego'),\n",
       " (' swoja', ' swoje'),\n",
       " (' swojej', ' swojego'),\n",
       " (' swojego', ' swojej'),\n",
       " ('kot', 'kot'),\n",
       " (' swej', ' swoim'),\n",
       " (' swoje', ' swojemu'),\n",
       " (' swym', ' swojej')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = 11, 12\n",
    "analyze_weights_vo(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=False,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tobie, ciebie, twoimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 20, Head 13\n",
      "one more iteration. 358\n",
      "one more iteration. 11868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' jej', ' jej'),\n",
       " (' jej', ' ona'),\n",
       " (' Ciebie', ' Ciebie'),\n",
       " (' jej', ' Ona'),\n",
       " (' wjej', ' jej'),\n",
       " (' Ciebie', ' Tobie'),\n",
       " (' ciebie', ' ciebie'),\n",
       " (' Twoich', ' Ciebie'),\n",
       " (' ich', ' ich'),\n",
       " (' ciebie', ' tobie'),\n",
       " (' Ciebie', ' Twojej'),\n",
       " (' Twojej', ' Ciebie'),\n",
       " (' Ciebie', ' Twoich'),\n",
       " (' twoimi', ' ciebie'),\n",
       " (' wjej', ' ona'),\n",
       " (' Twoim', ' Ciebie'),\n",
       " (' Ciebie', ' Twoim'),\n",
       " (' twoich', ' ciebie'),\n",
       " (' Twoje', ' Ciebie'),\n",
       " (' twoje', ' ciebie'),\n",
       " (' jej', ' wjej'),\n",
       " (' Ciebie', ' Twoje'),\n",
       " (' Twoich', ' Tobie'),\n",
       " (' wjej', ' Ona'),\n",
       " (' Tobie', ' Ciebie'),\n",
       " (' Twoich', ' Twoich'),\n",
       " (' ciebie', ' twoich'),\n",
       " (' Twoich', ' Twojej'),\n",
       " (' Ci', ' Ciebie'),\n",
       " (' ciebie', ' twoimi'),\n",
       " (' twoim', ' ciebie'),\n",
       " (' Twoim', ' Twoich'),\n",
       " (' ciebie', ' twojej'),\n",
       " (' was', ' was'),\n",
       " (' ciebie', ' twoim'),\n",
       " (' twoimi', ' tobie'),\n",
       " (' jej', ' niej'),\n",
       " (' jej', ' Jej'),\n",
       " (' was', ' wami'),\n",
       " (' twoich', ' tobie'),\n",
       " (' tobie', ' ciebie'),\n",
       " (' Twoich', ' Twoje'),\n",
       " (' twoich', ' twoich'),\n",
       " (' Twojej', ' Twoich'),\n",
       " (' twoimi', ' twoich'),\n",
       " (' twoje', ' tobie'),\n",
       " (' twoimi', ' twoimi'),\n",
       " (' Twojej', ' Tobie'),\n",
       " (' twojej', ' ciebie'),\n",
       " (' Tobie', ' Tobie')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = 20, 13\n",
    "analyze_weights_vo(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=False,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spjniki opisujce pooenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 19, Head 7\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 374\n",
      "one more iteration. 34965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' czele', ' na'),\n",
       " (' pryzmat', ' przez'),\n",
       " (' froncie', ' na'),\n",
       " (' etapie', ' na'),\n",
       " (' pewno', ' na'),\n",
       " ('miast', ' na'),\n",
       " (' powodu', ' z'),\n",
       " (' ekranie', ' na'),\n",
       " (' pryzmat', 'przez'),\n",
       " ('rzuca', ' do'),\n",
       " (' posiedzeniu', ' na'),\n",
       " (' tarasie', ' na'),\n",
       " ('ping', ' do'),\n",
       " ('ornie', ' przez'),\n",
       " (' smyczy', ' na'),\n",
       " (' poziomie', ' na'),\n",
       " ('ornie', ' przeze'),\n",
       " (' razu', ' od'),\n",
       " (' Wyspach', ' na'),\n",
       " (' dawna', ' od'),\n",
       " ('ted', ' w'),\n",
       " (' perspektywie', ' w'),\n",
       " (' terenach', ' na'),\n",
       " (' momencie', ' w'),\n",
       " ('orne', ' przez'),\n",
       " (' francusku', ' po'),\n",
       " (' ziemiach', ' na'),\n",
       " (' rezultacie', ' w'),\n",
       " (' wymiarze', ' w'),\n",
       " (' Twitterze', ' na'),\n",
       " (' barkach', ' na'),\n",
       " ('gnieniu', ' w'),\n",
       " ('leg', ' na'),\n",
       " (' mgnieniu', ' we'),\n",
       " (' posiedzeniach', ' na'),\n",
       " (' kresu', ' u'),\n",
       " (' mgnieniu', ' w'),\n",
       " (' razie', ' na'),\n",
       " (' rana', ' od'),\n",
       " (' pryzmat', ' przeze'),\n",
       " ('tedy', ' we'),\n",
       " ('ted', ' we'),\n",
       " (' podstawie', ' na'),\n",
       " (' sensie', ' w'),\n",
       " ('ornie', 'przez'),\n",
       " (' niemiecku', ' po'),\n",
       " (' rynkach', ' na'),\n",
       " ('jazdu', ' do'),\n",
       " (' Facebooku', ' na'),\n",
       " (' efekcie', ' w')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = 19, 7\n",
    "analyze_weights_vo(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=False,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spjniki inaczej pisane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 19, Head 1\n",
      "one more iteration. 0\n",
      "one more iteration. 2\n",
      "one more iteration. 639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' przed', ' przed'),\n",
       " (' nad', ' nad'),\n",
       " (' miedzy', ' miedzy'),\n",
       " ('tylko', 'tylko'),\n",
       " (' przed', 'przed'),\n",
       " ('poza', 'poza'),\n",
       " (' poza', ' poza'),\n",
       " (' nad', 'nad'),\n",
       " (' sam', ' sam'),\n",
       " (' pod', ' pod'),\n",
       " ('Nad', ' nad'),\n",
       " (' osobna', ' osobna'),\n",
       " ('przed', ' przed'),\n",
       " (' nad', 'Nad'),\n",
       " (' osobno', ' osobno'),\n",
       " ('poza', ' poza'),\n",
       " (' dalej', ' dalej'),\n",
       " ('tylko', ' tylko'),\n",
       " (' sama', ' sam'),\n",
       " (' poza', 'poza'),\n",
       " ('tylko', ' jeno'),\n",
       " (' same', ' same'),\n",
       " (' sam', ' sama'),\n",
       " (' najmocniej', ' najmocniej'),\n",
       " (' przed', 'Przed'),\n",
       " (' osobno', ' oddzielnie'),\n",
       " (' sama', ' sama'),\n",
       " (' oddzielnie', ' osobno'),\n",
       " ('Nad', 'nad'),\n",
       " (' same', ' sam'),\n",
       " (' najszybciej', ' najszybciej'),\n",
       " ('Nad', 'Nad'),\n",
       " (' dopiero', ' dopiero'),\n",
       " (' osobno', ' osobna'),\n",
       " (' same', ' sama'),\n",
       " ('przed', 'przed'),\n",
       " (' oddzielnie', ' oddzielnie'),\n",
       " (' osobna', ' osobno'),\n",
       " (' najdalej', ' najdalej'),\n",
       " (' kto', ' kto'),\n",
       " (' nade', ' nad'),\n",
       " (' razem', ' razem'),\n",
       " (' osobna', ' oddzielnie'),\n",
       " ('tylko', ' jedynie'),\n",
       " ('nad', ' nad'),\n",
       " (' najmniej', ' najmniej'),\n",
       " ('tylko', ' TYLKO'),\n",
       " (' przed', ' Przed'),\n",
       " (' od', ' od'),\n",
       " ('nad', 'nad')]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = 19, 1\n",
    "analyze_weights_vo(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=False,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inne..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 21, Head 4\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 5\n",
      "one more iteration. 17496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('zjaty', ' A'),\n",
       " ('rycznie', ' Li'),\n",
       " (' akcyz', ' opodat'),\n",
       " (' podatku', ' podatek'),\n",
       " ('onel', ' Li'),\n",
       " ('steri', ' M'),\n",
       " (' podatkach', ' podatki'),\n",
       " ('portu', ' Ra'),\n",
       " (' podatku', ' podatku'),\n",
       " ('owisk', ' R'),\n",
       " (' podatkach', ' podatkach'),\n",
       " (' podatkach', ' podatek'),\n",
       " ('nonim', ' A'),\n",
       " ('garnia', ' O'),\n",
       " ('zon', ' D'),\n",
       " ('mami', ' O'),\n",
       " ('gresyw', ' A'),\n",
       " (' akcyz', ' opodatkowania'),\n",
       " (' koszt', ' koszt'),\n",
       " ('aryka', ' K'),\n",
       " ('tery', ' Li'),\n",
       " ('boli', ' A'),\n",
       " ('zmat', ' G'),\n",
       " (' Saudyjskiej', ' Li'),\n",
       " ('niowym', ' Li'),\n",
       " ('warantan', ' K'),\n",
       " (' podatkach', ' podatku'),\n",
       " (' podatku', ' podatki'),\n",
       " ('aliz', ' R'),\n",
       " (' akcyz', ' podatek'),\n",
       " ('wagi', ' U'),\n",
       " ('ure', ' Ma'),\n",
       " ('pelu', ' Ka'),\n",
       " ('niowej', ' Li'),\n",
       " ('gazy', ' Ma'),\n",
       " ('rygi', ' O'),\n",
       " ('ryczny', ' Li'),\n",
       " (' akcyz', ' podatkowych'),\n",
       " (' podatkiem', ' opodat'),\n",
       " (' podatku', ' podatkowym'),\n",
       " ('seum', ' Li'),\n",
       " ('et', ' R'),\n",
       " (' koszty', ' koszty'),\n",
       " (' akcyz', ' podatkowe'),\n",
       " (' akcyz', ' podatku'),\n",
       " (' podatek', ' podatek'),\n",
       " (' akcyz', ' podatkiem'),\n",
       " ('beli', ' O'),\n",
       " (' czynszu', ' stawki'),\n",
       " (' podatku', ' podatkach')]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = np.random.randint(num_layers), np.random.randint(num_heads)\n",
    "analyze_weights_vo(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=False,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WQK Interpretation\n",
    "Interpretacja Wqk polega na analizie tej macierzy, identyfikujc pary sw, ktre silnie ze sob wspgraj w kontekcie mechanizmu uwagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_weigths_qk(i1, i2, exclude_same=False, reverse_list=False, only_ascii=True, only_alnum=True, tokens_list=None):\n",
    "    print(f\"Layer {i1}, Head {i2}\")\n",
    "\n",
    "    # Extract the weights for the specified layer and neuron\n",
    "    W_Q_tmp, W_K_tmp = W_Q_heads[i1, i2, :], W_K_heads[i1, i2, :]\n",
    "\n",
    "    # Compute the temporary matrix using emb_inv and emb_inv.T\n",
    "    tmp = emb_inv @ (W_Q_tmp @ W_K_tmp.T) @ emb_inv.T\n",
    "\n",
    "    # Approximate top-k values\n",
    "    all_high_pos = approx_topk(tmp, th0=1, verbose=True)\n",
    "\n",
    "    # Get top entries based on the specified parameters\n",
    "    top_entries = get_top_entries(\n",
    "        tmp,\n",
    "        all_high_pos,\n",
    "        only_ascii=only_ascii,\n",
    "        only_alnum=only_alnum,\n",
    "        exclude_same=exclude_same,\n",
    "        tokens_list=tokens_list,\n",
    "        reverse_list=reverse_list\n",
    "    )\n",
    "\n",
    "    return top_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 21, Head 7\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 42\n",
      "one more iteration. 316744\n",
      "one more iteration. 4556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' za', 'noscia'),\n",
       " ('dnieniem', ' przy'),\n",
       " (' pod', 'Uwiel'),\n",
       " (' w', 'noscia'),\n",
       " (' w', 'kowicz'),\n",
       " ('aryjnych', ' tylko'),\n",
       " ('aryjnych', ' po'),\n",
       " (' w', 'knal'),\n",
       " (' z', 'noscia'),\n",
       " (' w', 'dagogi'),\n",
       " (' przy', 'owienia'),\n",
       " (' i', 'noscia'),\n",
       " (' pod', 'cjonista'),\n",
       " (' pod', 'nujesz'),\n",
       " (' nad', 'ngu'),\n",
       " (' przy', 'Pry'),\n",
       " ('jutrz', ' w'),\n",
       " (' pod', 'alski'),\n",
       " (' w', 'lujesz'),\n",
       " (' przy', ' budowlane'),\n",
       " ('aryjnych', ' przy'),\n",
       " (' na', 'noscia'),\n",
       " ('aryjnych', ' nawet'),\n",
       " ('Przytak', ' przy'),\n",
       " (' na', 'owepaski'),\n",
       " (' po', 'bil'),\n",
       " (' od', 'dagogi'),\n",
       " (' w', 'URWIA'),\n",
       " (' pod', ' glowa'),\n",
       " (' po', 'sywnie'),\n",
       " (' do', 'wychw'),\n",
       " ('aryjnych', ' zak'),\n",
       " (' nad', 'obior'),\n",
       " (' pod', 'niuje'),\n",
       " (' pod', 'zc'),\n",
       " (' w', 'towaniem'),\n",
       " (' pod', 'Przytu'),\n",
       " (' przed', 'ingiem'),\n",
       " (' o', 'bino'),\n",
       " (' w', 'tujesz'),\n",
       " (' do', 'ariatu'),\n",
       " (' dla', ' pryzmat'),\n",
       " (' pod', ' Wojskowego'),\n",
       " (' po', 'dujesz'),\n",
       " (' i', 'cjonista'),\n",
       " (' z', 'kowicz'),\n",
       " (' na', 'dagogi'),\n",
       " (' w', 'ustu'),\n",
       " (' o', 'noscia'),\n",
       " ('mitha', ' tylko')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer, head = 21, 7\n",
    "analyze_weigths_qk(\n",
    "    layer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=True,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podre linie lotnicze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, Head 5\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 14860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' konsulat', ' rzymski'),\n",
       " (' interpretacje', ' podatkowych'),\n",
       " (' liniach', ' lotnicze'),\n",
       " (' pokoju', ' hotel'),\n",
       " (' linii', ' tramwaju'),\n",
       " (' liniach', ' lotniczy'),\n",
       " (' liniach', ' lotniczych'),\n",
       " (' kurator', ' sztuki'),\n",
       " (' liniach', ' tramwaju'),\n",
       " (' terminalu', ' lotni'),\n",
       " (' kuratora', ' sztuki'),\n",
       " (' konsulat', ' Rzymi'),\n",
       " (' terminalu', ' lotniczych'),\n",
       " (' interpretacji', ' podatkowych'),\n",
       " (' interpretacje', ' podatkowego'),\n",
       " (' linie', ' lotnicze'),\n",
       " (' liniach', ' lotniczego'),\n",
       " (' liniach', ' tramwaj'),\n",
       " (' liniach', ' samolo'),\n",
       " (' liniach', ' tramwaje'),\n",
       " (' liniach', ' tramwa'),\n",
       " (' liniach', ' samolocie'),\n",
       " (' liniach', ' kolejowa'),\n",
       " (' konsulat', ' Cezara'),\n",
       " (' linii', ' kolejowa'),\n",
       " (' linie', ' lotniczej'),\n",
       " (' linie', ' lotniczy'),\n",
       " (' linie', ' lotniczych'),\n",
       " (' linie', ' samolo'),\n",
       " (' interpretacje', ' podatkowe'),\n",
       " (' linie', ' kolejowe'),\n",
       " (' linii', 'Samolot'),\n",
       " (' terminalu', ' lotnicze'),\n",
       " (' linie', 'Samolot'),\n",
       " (' linii', ' lotniczych'),\n",
       " (' interpretacji', ' podatkowego'),\n",
       " (' linii', ' tramwaje'),\n",
       " (' linie', ' lot'),\n",
       " (' liniach', ' lotni'),\n",
       " (' linie', ' kolejowa'),\n",
       " (' linii', ' lotnicze'),\n",
       " (' liniach', ' kolejowe'),\n",
       " (' liniach', ' autobus'),\n",
       " (' linie', ' tramwaju'),\n",
       " (' linii', ' samolo'),\n",
       " (' konsulat', ' rzymskich'),\n",
       " (' terminalu', ' lotniczy'),\n",
       " (' liniach', ' LOT'),\n",
       " (' terminalu', ' lotniczego'),\n",
       " (' linie', ' tramwa')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = 0, 5 \n",
    "analyze_weigths_qk(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=True,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pan, panem, pani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 23, Head 15\n",
      "one more iteration. 0\n",
      "one more iteration. 573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' panem', ' pan'),\n",
       " (' pana', ' pan'),\n",
       " (' pan', ' panu'),\n",
       " (' pana', ' panu'),\n",
       " (' panem', ' panu'),\n",
       " (' pan', ' pan'),\n",
       " (' panem', ' pana'),\n",
       " (' pana', ' pana'),\n",
       " (' pan', ' pana'),\n",
       " (' panu', ' panu'),\n",
       " (' Pani', ' Pani'),\n",
       " ('Pana', ' pan'),\n",
       " (' pana', ' Panu'),\n",
       " (' pan', ' Panu'),\n",
       " (' panu', ' pan'),\n",
       " ('panem', ' pan'),\n",
       " (' Panem', ' pan'),\n",
       " (' pana', ' Pan'),\n",
       " (' pani', ' Pani'),\n",
       " (' pana', 'pan'),\n",
       " (' pana', ' panowie'),\n",
       " (' panem', ' Pan'),\n",
       " (' pan', 'pan'),\n",
       " (' pan', ' panem'),\n",
       " (' panu', ' pana'),\n",
       " (' pan', ' Pan'),\n",
       " ('Pani', ' pani'),\n",
       " (' pana', ' Pana'),\n",
       " ('Pani', ' Pani'),\n",
       " ('panem', ' panu'),\n",
       " (' pan', ' Pana'),\n",
       " (' panem', ' Panu'),\n",
       " (' pana', ' panem'),\n",
       " (' pan', ' panom'),\n",
       " ('Pana', ' pana'),\n",
       " ('Pana', ' panu'),\n",
       " (' Pana', ' pan'),\n",
       " ('Pan', ' pan'),\n",
       " (' panom', ' pan'),\n",
       " ('Pan', ' pana'),\n",
       " (' Panem', ' panu'),\n",
       " (' Panem', ' pana'),\n",
       " (' panem', ' Pana'),\n",
       " (' pani', ' pani'),\n",
       " (' pana', ' panom'),\n",
       " (' panem', ' panem'),\n",
       " ('panem', ' pana'),\n",
       " ('Pan', ' panu'),\n",
       " (' panu', ' Panu'),\n",
       " (' Panu', ' pan')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = 23, 15\n",
    "analyze_weigths_qk(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=True,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formy grzecznociowe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 8, Head 1\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 67\n",
      "one more iteration. 39053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' gdybym', ' gdybym'),\n",
       " (' zapraszam', ' jakbym'),\n",
       " (' moimi', ' jakbym'),\n",
       " (' gdybym', ' jakbym'),\n",
       " (' zapraszam', ' mojej'),\n",
       " (' abym', ' jakbym'),\n",
       " (' jakbym', ' jakbym'),\n",
       " (' nacjona', 'ownictwo'),\n",
       " ('erytory', ' percep'),\n",
       " (' moich', ' jakbym'),\n",
       " ('erytory', 'zumiem'),\n",
       " (' abym', 'lewam'),\n",
       " (' zapraszam', ' umiem'),\n",
       " ('interpre', 'alizmu'),\n",
       " (' zapraszam', ' moja'),\n",
       " (' moimi', ' moich'),\n",
       " (' czytasz', 'rod'),\n",
       " (' zapraszam', ' czekam'),\n",
       " (' prosimy', ' znajdziecie'),\n",
       " (' jakbym', ' gdybym'),\n",
       " (' moimi', ' gdybym'),\n",
       " (' skutkiem', ' prania'),\n",
       " (' abym', ' abym'),\n",
       " (' abym', ' gdybym'),\n",
       " (' kryzysu', ' wojennego'),\n",
       " (' zapraszam', ' gdybym'),\n",
       " (' jakbym', 'lewam'),\n",
       " (' zapraszam', ' moich'),\n",
       " (' mylisz', ' waszej'),\n",
       " ('erytory', ' wieczystych'),\n",
       " (' jakbym', ' umiem'),\n",
       " ('erytory', 'rach'),\n",
       " (' moimi', ' abym'),\n",
       " (' prosze', ' jakbym'),\n",
       " ('sen', ' tvn'),\n",
       " (' uszczerbku', ' prania'),\n",
       " (' zapraszam', ' szukam'),\n",
       " (' moimi', 'lewam'),\n",
       " (' wasza', ' waszych'),\n",
       " ('gnu', 'rod'),\n",
       " (' moich', ' gdybym'),\n",
       " ('Kod', 'DU'),\n",
       " ('logi', 'archi'),\n",
       " (' zapraszam', ' trafisz'),\n",
       " (' moimi', ' moimi'),\n",
       " (' prosze', ' trafisz'),\n",
       " (' jakbym', ' staram'),\n",
       " (' zapraszam', 'trzebu'),\n",
       " ('lator', 'laty'),\n",
       " (' kryzysu', ' niezadowolenia')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = 8, 1\n",
    "analyze_weigths_qk(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=True,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spjniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 17, Head 15\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 46\n",
      "one more iteration. 110979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' itp', ' etc'),\n",
       " (' itp', ' itp'),\n",
       " (' itp', ' itd'),\n",
       " (' etc', ' etc'),\n",
       " (' itd', ' itd'),\n",
       " (' itd', ' etc'),\n",
       " (' ktora', ' jesli'),\n",
       " (' ktore', ' jesli'),\n",
       " (' etc', ' itp'),\n",
       " (' itd', ' itp'),\n",
       " (' etc', ' itd'),\n",
       " (' ktorzy', ' jesli'),\n",
       " (' jezeli', ' jesli'),\n",
       " (' ktore', ' jezeli'),\n",
       " (' gdyz', ' jesli'),\n",
       " (' ktora', ' jezeli'),\n",
       " (' jezeli', ' jezeli'),\n",
       " (' ktory', ' jesli'),\n",
       " (' poniewaz', ' jesli'),\n",
       " (' ktorym', ' jesli'),\n",
       " (' gdyz', ' poniewaz'),\n",
       " (' gdyz', ' gdyz'),\n",
       " (' ktory', ' jezeli'),\n",
       " (' ktorych', ' jesli'),\n",
       " (' ktora', ' choc'),\n",
       " (' gdyz', ' choc'),\n",
       " (' ktore', ' choc'),\n",
       " (' jesli', ' jesli'),\n",
       " (' ktorzy', ' jezeli'),\n",
       " (' gdyz', ' gdy'),\n",
       " (' zeby', ' jesli'),\n",
       " (' poniewaz', ' poniewaz'),\n",
       " (' ktorym', ' jezeli'),\n",
       " (' ktor', ' jesli'),\n",
       " ('plementa', ' i'),\n",
       " (' ktor', ' jezeli'),\n",
       " (' troche', ' ktorych'),\n",
       " (' poniewaz', ' jezeli'),\n",
       " (' wiecej', ' ktorych'),\n",
       " (' zeby', ' jezeli'),\n",
       " (' kogos', ' ktorych'),\n",
       " (' jakiegos', ' zeby'),\n",
       " (' ktorego', ' jesli'),\n",
       " (' jesli', ' jezeli'),\n",
       " (' jakies', ' ktorych'),\n",
       " (' ktore', ' ktore'),\n",
       " (' ktorej', ' jesli'),\n",
       " (' gdzies', ' jakims'),\n",
       " (' skad', ' jesli'),\n",
       " (' kogos', ' mysli')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = 17, 15\n",
    "analyze_weigths_qk(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=True,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, Head 12\n",
      "one more iteration. 0\n",
      "one more iteration. 262\n",
      "one more iteration. 27303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wala', 'la'),\n",
       " ('walam', 'la'),\n",
       " (' City', ' Schengen'),\n",
       " (' stacji', ' Schengen'),\n",
       " ('lam', 'la'),\n",
       " (' aplikacji', ' Schengen'),\n",
       " (' ja', 'la'),\n",
       " (' przejazdu', ' Schengen'),\n",
       " (' lokalizacji', ' socjologii'),\n",
       " (' trasy', ' turystycznej'),\n",
       " (' lokalizacji', ' policj'),\n",
       " (' lokalizacji', ' Schengen'),\n",
       " (' stacji', ' policj'),\n",
       " (' akcji', ' turystycznej'),\n",
       " (' inwestycji', ' Schengen'),\n",
       " ('wal', 'la'),\n",
       " (' kolonii', ' Schengen'),\n",
       " (' terenu', ' policj'),\n",
       " (' lokalizacji', ' turystyczny'),\n",
       " (' lokalizacji', ' depor'),\n",
       " ('gla', 'la'),\n",
       " (' lokalizacji', ' terenowych'),\n",
       " (' przejazdu', ' terroryzmu'),\n",
       " (' operacji', ' Schengen'),\n",
       " (' przejazdu', ' turystycznej'),\n",
       " ('ala', 'la'),\n",
       " (' lokalizacji', ' turystycznej'),\n",
       " (' oprogramowania', ' socjologii'),\n",
       " (' lokalizacji', ' zakwater'),\n",
       " (' przejazdu', ' policj'),\n",
       " (' nia', 'la'),\n",
       " (' inwestycji', ' turystycznej'),\n",
       " (' przejazdu', ' depor'),\n",
       " (' lokalizacji', ' spado'),\n",
       " ('scie', 'la'),\n",
       " (' inwestycji', ' dyplomatycznych'),\n",
       " (' aplikacji', ' socjologii'),\n",
       " (' akcji', ' turystyczny'),\n",
       " (' inwestycji', ' zbiorowych'),\n",
       " (' akcji', ' Schengen')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilayer, head = np.random.randint(num_layers), np.random.randint(num_heads)\n",
    "analyze_weigths_qk(\n",
    "    ilayer,\n",
    "    head,\n",
    "    exclude_same=False,\n",
    "    reverse_list=False,\n",
    "    only_ascii=True,\n",
    "    only_alnum=True,\n",
    "    tokens_list=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analiza Wvo i Wqk w przestrzeni osadze jak analiza ukrytych stanw lub wag funkcji Feed Forward, mog dostarczy cennych informacji na temat dziaania modelu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
